{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n",
    "from dataclasses import dataclass\n",
    "from pprint import pformat\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the npy file\n",
    "# You can use numpy's load function to read .npy files\n",
    "npy_data : np.ndarray= np.load('./weights.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))  # Numerical stability\n",
    "    return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "def softmax_derivative(softmax_output):\n",
    "    # The derivative of softmax is generally used with cross-entropy loss, but for this example:\n",
    "    s = softmax_output.reshape(-1, 1)\n",
    "    return np.diagflat(s) - np.dot(s, s.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tools:\n",
    "    activate_funcs = {\n",
    "        \"sigmoid\": (\n",
    "            lambda x: 1 / (1 + np.exp(-x)),\n",
    "            lambda x: 1 / (1 + np.exp(-x) ** 2),\n",
    "        ),\n",
    "        \"relu\": (lambda x: np.maximum(0, x), lambda x: np.where(x > 0, 1, 0)),\n",
    "        \"leaky_relu\": (\n",
    "            lambda x: np.where(x > 0, x, x * 0.01),\n",
    "            lambda x: np.where(x > 0, 1, 0.01),\n",
    "        ),\n",
    "        \"tanh\": (lambda x: np.tanh(x), lambda x: 1 - np.tanh(x) ** 2),\n",
    "        \"linear\": (lambda x: x, lambda x: 1),\n",
    "        \"softmax\": (\n",
    "            lambda x: np.clip(softmax(x), 1e-15, 1 - 1e-15),\n",
    "            softmax_derivative,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    loss_funcs = {\n",
    "        \"sse\": (\n",
    "            lambda y, y_hat: np.sum((y - y_hat) ** 2),\n",
    "            lambda y, y_hat: -2 * (y - y_hat),\n",
    "        ),\n",
    "        \"rms\": (\n",
    "            lambda y, y_hat: np.sqrt(np.mean((y - y_hat) ** 2)),\n",
    "            lambda y, y_hat: -(y - y_hat)\n",
    "            / (len(y) * np.sqrt(np.sum((y - y_hat) ** 2))),\n",
    "        ),\n",
    "        \"crossentropy\": (\n",
    "            lambda y, y_hat: -np.sum(y * np.log(y_hat + 1e-15)),\n",
    "            lambda y, y_hat: y_hat - y,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def activate_func(name: str) -> tuple[Callable, Callable]:\n",
    "        return Tools.activate_funcs[name]\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_func(name: str) -> tuple[Callable, Callable]:\n",
    "        return Tools.loss_funcs[name]\n",
    "\n",
    "    @staticmethod\n",
    "    def rms(y_true: np.ndarray, y_hat: np.ndarray) -> float:\n",
    "        return np.sqrt(np.mean(np.sum(y_true - y_hat) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Layer:\n",
    "    input_node: int\n",
    "    activate: str\n",
    "    output_node: int\n",
    "    dropout_rate: float = 0.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseModelLayer:\n",
    "    w: np.ndarray\n",
    "    b: np.ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def zero_like(cls, layer_basic):\n",
    "        w = np.zeros_like(layer_basic.w)\n",
    "        b = np.zeros_like(layer_basic.b)\n",
    "        return cls(w, b)\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> dict:\n",
    "        return {\"w\": self.w.shape, \"b\": self.b.shape}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelLayer(BaseModelLayer):\n",
    "    activate_str: str\n",
    "    activate: Callable[[np.ndarray], np.ndarray]\n",
    "    activate_derivative: Callable[[np.ndarray], np.ndarray]\n",
    "    dropout_rate: float\n",
    "\n",
    "    @classmethod\n",
    "    def load_weights(cls, w:np.ndarray, b:np.ndarray, activate_str:str, dropout_rate: float = 0.0):\n",
    "        activate, activate_derivative = Tools.activate_func(activate_str)\n",
    "        \n",
    "        return cls(\n",
    "            w=w,\n",
    "            b=b,\n",
    "            dropout_rate=dropout_rate,\n",
    "            activate_str=activate_str,\n",
    "            activate=activate,\n",
    "            activate_derivative=activate_derivative\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def build_layer(cls, layer_config: Layer):\n",
    "        if not (0 <= layer_config.dropout_rate < 1):\n",
    "            raise ValueError(\"dropout_rate must between 0 and 1\")\n",
    "\n",
    "        w = np.random.randn(layer_config.output_node, layer_config.input_node)\n",
    "        b = np.random.randn(layer_config.output_node, 1)\n",
    "\n",
    "        w, b = (\n",
    "            w * np.sqrt(1.0 / layer_config.input_node),\n",
    "            b * 0.01,\n",
    "        )\n",
    "        activate, activate_derivative = Tools.activate_func(layer_config.activate)\n",
    "        return cls(\n",
    "            w=w,\n",
    "            b=b,\n",
    "            dropout_rate=layer_config.dropout_rate,\n",
    "            activate_str=layer_config.activate,\n",
    "            activate=activate,\n",
    "            activate_derivative=activate_derivative,\n",
    "        )\n",
    "\n",
    "    def forward_drop_out(self, a: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        mask = np.random.binomial(1, 1 - self.dropout_rate, size=a.shape)\n",
    "        a_drop_out = (a * mask) / (1 - self.dropout_rate)\n",
    "        return a_drop_out, mask\n",
    "\n",
    "    def update_delta(\n",
    "        self,\n",
    "        delta: np.ndarray,\n",
    "        layer_next,\n",
    "        z_next: np.ndarray,\n",
    "        mask: np.ndarray = None,\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "        if mask is not None:\n",
    "            delta *= mask\n",
    "\n",
    "        return (self.w.T @ delta) * layer_next.activate_derivative(z_next)\n",
    "\n",
    "    def update(self, grad_item: BaseModelLayer, lr: float) -> None:\n",
    "        assert (\n",
    "            self.w.shape == grad_item.w.shape\n",
    "        ), f\"权重形状不匹配: {self.w.shape} vs {grad_item.w.shape}\"\n",
    "        assert self.b.shape[1] == 1\n",
    "\n",
    "        assert (\n",
    "            self.b.shape == grad_item.b.shape\n",
    "        ), f\"偏置形状不匹配: {self.b.shape} vs {grad_item.b.shape} {grad_item.w.shape}\"\n",
    "\n",
    "        self.w -= lr * grad_item.w\n",
    "        self.b -= lr * grad_item.b\n",
    "        return\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        z = self.w @ x + self.b\n",
    "        a = self.activate(z)\n",
    "        return z, a\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"w: {self.w.shape}, b: {self.b.shape}, activate: {self.activate_str}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self, layers_config: list[Layer | ModelLayer], loss_func: str = \"sse\"):\n",
    "        \n",
    "        self._input_dim, self._output_dim, self._layers = DNN._dim_layers(layers_config)\n",
    "\n",
    "        self._layers_len = len(self._layers)\n",
    "\n",
    "        self._loss_func, self._loss_derivative = Tools.loss_func(loss_func)\n",
    "\n",
    "        # self._is_multi = self._output_dim > 1\n",
    "\n",
    "        self._model_struct = {\n",
    "            \"input_dim\": self._input_dim,\n",
    "            \"output_dim\": self._output_dim,\n",
    "            \"hidden_layers\": self._layers,\n",
    "            \"loss_func\": loss_func,\n",
    "        }\n",
    "\n",
    "        return\n",
    "    @staticmethod\n",
    "    def _dim_layers(layers_config: list[Layer | ModelLayer]) -> tuple[int, int, dict[int, ModelLayer]]:\n",
    "        sample = layers_config[0]\n",
    "        input_dim , output_dim = None, None\n",
    "        \n",
    "        layers = {}\n",
    "        \n",
    "        if isinstance(sample, Layer):\n",
    "            input_dim , output_dim = (\n",
    "                layers_config[0].input_node,\n",
    "                layers_config[-1].output_node,\n",
    "            )\n",
    "            \n",
    "            layers = {\n",
    "                i: ModelLayer.build_layer(layer)\n",
    "                for i, layer in enumerate(layers_config, start=1)\n",
    "            }\n",
    "            \n",
    "        elif isinstance(sample, ModelLayer):\n",
    "            input_dim , output_dim = (\n",
    "                layers_config[0].w.shape[1],\n",
    "                layers_config[-1].w.shape[0],\n",
    "            )\n",
    "            \n",
    "            layers = {\n",
    "                i: layer\n",
    "                for i, layer in enumerate(layers_config, start=1)\n",
    "            }\n",
    "\n",
    "        return input_dim, output_dim, layers \n",
    "    \n",
    "    @property\n",
    "    def model_struct(self):\n",
    "        return self._model_struct\n",
    "\n",
    "    @property\n",
    "    def w(self):\n",
    "        return self._w\n",
    "\n",
    "    @property\n",
    "    def b(self):\n",
    "        return self._b\n",
    "\n",
    "    def forward(\n",
    "        self, x: np.ndarray, for_backward: bool = False\n",
    "    ) -> (\n",
    "        tuple[dict[int, np.ndarray], dict[int, np.ndarray], dict[int, np.ndarray]]\n",
    "        | np.ndarray\n",
    "    ):\n",
    "\n",
    "        if len(x.shape) == 1:\n",
    "            x = x[:, np.newaxis]\n",
    "\n",
    "        a_out, z_out, mask_out = {0: x}, dict(), dict()\n",
    "\n",
    "        for i, layer in self._layers.items():\n",
    "            z_out[i], a_out[i] = layer(a_out[i - 1])\n",
    "\n",
    "            if layer.dropout_rate > 0 and for_backward:\n",
    "                a_out[i], mask_out[i] = layer.forward_drop_out(a_out[i])\n",
    "\n",
    "        last_item = next(reversed(a_out.values()))\n",
    "\n",
    "        if for_backward:\n",
    "            return last_item, a_out, z_out, mask_out\n",
    "\n",
    "        return last_item\n",
    "\n",
    "    def init_delta(self, a_out: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        # if self._is_multi:\n",
    "\n",
    "        #     return self._loss_derivative(y, softmax(a_out)) / y.shape[1]\n",
    "        # print(y.shape)\n",
    "        return self._loss_derivative(y, a_out) / y.shape[1]\n",
    "\n",
    "    def backward(\n",
    "        self,\n",
    "        end_y_hat: np.ndarray,\n",
    "        a_out: dict[int, np.ndarray],\n",
    "        z_out: dict[int, np.ndarray],\n",
    "        mask_out: dict[int, np.ndarray],\n",
    "        y: np.ndarray,\n",
    "    ) -> dict[int, BaseModelLayer]:\n",
    "        # https://medium.com/@erikhallstrm/backpropagation-from-the-beginning-77356edf427d\n",
    "        \"\"\"\n",
    "        put y data is a column vector , so if data is row vector , please transpose\n",
    "        \"\"\"\n",
    "        grad = {i: BaseModelLayer.zero_like(layer) for i, layer in self._layers.items()}\n",
    "\n",
    "        # init the delta\n",
    "        delta = self.init_delta(end_y_hat, y)\n",
    "        # print({\"delta\": delta, \"y_hat\": end_y_hat, \"y\": y})\n",
    "\n",
    "        for layer_index in reversed(self._layers.keys()):\n",
    "            next_index = layer_index - 1\n",
    "\n",
    "            grad[layer_index].w = delta @ a_out[next_index].T\n",
    "\n",
    "            grad[layer_index].b = np.sum(delta, axis=1, keepdims=True)\n",
    "\n",
    "            if layer_index > 1:\n",
    "                delta = self._layers[layer_index].update_delta(\n",
    "                    delta=delta,\n",
    "                    mask=mask_out.get(layer_index, None),\n",
    "                    layer_next=self._layers[next_index],\n",
    "                    z_next=z_out[next_index],\n",
    "                )\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def _update(self, grad: dict[int, BaseModelLayer], learning_rate: float) -> None:\n",
    "        for i, grad_item in grad.items():\n",
    "            self._layers[i].update(grad_item, lr=learning_rate)\n",
    "\n",
    "        return\n",
    "\n",
    "    def __call__(self, x) -> np.ndarray:\n",
    "        return self.forward(x.T, for_backward=False)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return pformat(self._model_struct)\n",
    "\n",
    "    def _l2_regularization(self, lambda_reg: float = 0.01):\n",
    "        weight_item = [np.sum(np.square(layer.w)) for layer in self._layers.values()]\n",
    "        return lambda_reg * 0.5 * np.sum(weight_item)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        epochs: int,\n",
    "        batch_size: int,\n",
    "        learning_rate: float,\n",
    "        l2: float = 0,\n",
    "        save_folder: str = None,\n",
    "    ) -> None:\n",
    "        loss_log = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            indices = np.random.permutation(x.shape[0])\n",
    "            X_shuffled = x[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for i in range(0, X_shuffled.shape[0], batch_size):\n",
    "\n",
    "                X_batch = X_shuffled[i : i + batch_size]\n",
    "                Y_batch = y_shuffled[i : i + batch_size]\n",
    "\n",
    "                x_batch = X_batch.T\n",
    "                y_batch = Y_batch.T\n",
    "\n",
    "                y_hat, a_out, z_out, mask_out = self.forward(x_batch, for_backward=True)\n",
    "\n",
    "                loss = self._loss_func(y_batch, y_hat)\n",
    "                grad = self.backward(y_hat, a_out, z_out, mask_out, y_batch)\n",
    "\n",
    "                if l2:\n",
    "                    loss += self._l2_regularization(l2)\n",
    "\n",
    "                    # update grad with l2\n",
    "                    for i in grad.keys():\n",
    "                        grad[i].w += l2 * self._layers[i].w\n",
    "\n",
    "                # update\n",
    "                self._update(grad, learning_rate)\n",
    "\n",
    "                loss_log.append({\"epoch\": epoch, \"loss\": loss})\n",
    "                logger.info(f\"epoch {epoch}, loss {loss}\")\n",
    "\n",
    "        if save_folder is not None:\n",
    "            path = Path(save_folder)\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            df_log = pd.DataFrame(loss_log)\n",
    "            df_log.to_csv(path.joinpath(\"loss.csv\"), index=False)\n",
    "\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(df_log.index, df_log[\"loss\"], marker=\"o\", label=\"Loss\")\n",
    "\n",
    "            # Labeling the plot\n",
    "            plt.xlabel(\"Index\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.title(\"Loss vs. Index\")\n",
    "            plt.legend()\n",
    "            plt.savefig(path.joinpath(\"loss.png\"))\n",
    "\n",
    "            logger.info(f\"save to {path}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    @classmethod\n",
    "    def lazy_build(\n",
    "        cls,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        calculate_layers: int,\n",
    "        activate_func: str,\n",
    "        up_dim: int,\n",
    "        loss_func: str = \"sse\",\n",
    "    ):\n",
    "\n",
    "        lazy_config = (\n",
    "            [Layer(input_node=input_dim, output_node=up_dim, activate=activate_func)]\n",
    "            + [\n",
    "                Layer(input_node=up_dim, output_node=up_dim, activate=activate_func)\n",
    "                for _ in range(calculate_layers - 2)\n",
    "            ]\n",
    "            + [Layer(input_node=up_dim, output_node=output_dim, activate=activate_func)]\n",
    "        )\n",
    "\n",
    "        return cls(lazy_config, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fnn(DNN):\n",
    "    def __init__(self, layers_config: list[Layer | ModelLayer], loss_func: str = \"sse\"):\n",
    "        super().__init__(layers_config, loss_func)\n",
    "        return\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_weights(path: str, loss_func: str = \"crossentropy\"):\n",
    "        model_weight = np.load(path, allow_pickle=True)\n",
    "        \n",
    "        model_dict = {\n",
    "            key : model_weight.item()[key] \n",
    "            for key in model_weight.item()\n",
    "        }\n",
    "        \n",
    "        layers_config = [\n",
    "            ModelLayer.load_weights(\n",
    "                w=model_dict[f\"w{i}\"].T,\n",
    "                b=model_dict[f\"b{i}\"],\n",
    "                activate_str=\"relu\",\n",
    "            )\n",
    "            for i in range(1, 4)\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        return Fnn(layers_config, loss_func)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Fnn.load_weights('./weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': {1: w: (2048, 784), b: (2048, 1), activate: relu,\n",
      "                   2: w: (512, 2048), b: (512, 1), activate: relu,\n",
      "                   3: w: (10, 512), b: (10, 1), activate: relu},\n",
      " 'input_dim': 784,\n",
      " 'loss_func': 'crossentropy',\n",
      " 'output_dim': 10}\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(np.random.randn(784,20).T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
