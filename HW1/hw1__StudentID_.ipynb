{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n",
    "from dataclasses import dataclass\n",
    "from pprint import pformat\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the npy file\n",
    "# You can use numpy's load function to read .npy files\n",
    "npy_data : np.ndarray= np.load('./weights.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))  # Numerical stability\n",
    "    return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "def softmax_derivative(softmax_output):\n",
    "    # The derivative of softmax is generally used with cross-entropy loss, but for this example:\n",
    "    s = softmax_output.reshape(-1, 1)\n",
    "    return np.diagflat(s) - np.dot(s, s.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tools:\n",
    "    activate_funcs = {\n",
    "        \"sigmoid\": (\n",
    "            lambda x: 1 / (1 + np.exp(-x)),\n",
    "            lambda x: 1 / (1 + np.exp(-x) ** 2),\n",
    "        ),\n",
    "        \"relu\": (lambda x: np.maximum(0, x), lambda x: np.where(x > 0, 1, 0)),\n",
    "        \"leaky_relu\": (\n",
    "            lambda x: np.where(x > 0, x, x * 0.01),\n",
    "            lambda x: np.where(x > 0, 1, 0.01),\n",
    "        ),\n",
    "        \"tanh\": (lambda x: np.tanh(x), lambda x: 1 - np.tanh(x) ** 2),\n",
    "        \"linear\": (lambda x: x, lambda x: 1),\n",
    "        \"softmax\": (\n",
    "            lambda x: np.clip(softmax(x), 1e-15, 1 - 1e-15),\n",
    "            softmax_derivative,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    loss_funcs = {\n",
    "        \"sse\": (\n",
    "            lambda y, y_hat: np.sum((y - y_hat) ** 2),\n",
    "            lambda y, y_hat: -2 * (y - y_hat),\n",
    "        ),\n",
    "        \"rms\": (\n",
    "            lambda y, y_hat: np.sqrt(np.mean((y - y_hat) ** 2)),\n",
    "            lambda y, y_hat: -(y - y_hat)\n",
    "            / (len(y) * np.sqrt(np.sum((y - y_hat) ** 2))),\n",
    "        ),\n",
    "        \"crossentropy\": (\n",
    "            lambda y, y_hat: -np.sum(y * np.log(y_hat + 1e-15)),\n",
    "            lambda y, y_hat: y_hat - y,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def activate_func(name: str) -> tuple[Callable, Callable]:\n",
    "        return Tools.activate_funcs[name]\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_func(name: str) -> tuple[Callable, Callable]:\n",
    "        return Tools.loss_funcs[name]\n",
    "\n",
    "    @staticmethod\n",
    "    def rms(y_true: np.ndarray, y_hat: np.ndarray) -> float:\n",
    "        return np.sqrt(np.mean(np.sum(y_true - y_hat) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Layer:\n",
    "    input_node: int\n",
    "    activate: str\n",
    "    output_node: int\n",
    "    dropout_rate: float = 0.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseModelLayer:\n",
    "    w: np.ndarray\n",
    "    b: np.ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def zero_like(cls, layer_basic):\n",
    "        w = np.zeros_like(layer_basic.w)\n",
    "        b = np.zeros_like(layer_basic.b)\n",
    "        return cls(w, b)\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> dict:\n",
    "        return {\"w\": self.w.shape, \"b\": self.b.shape}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelLayer(BaseModelLayer):\n",
    "    activate_str: str\n",
    "    activate: Callable[[np.ndarray], np.ndarray]\n",
    "    activate_derivative: Callable[[np.ndarray], np.ndarray]\n",
    "    dropout_rate: float\n",
    "\n",
    "    @classmethod\n",
    "    def build_layer(cls, layer_config: Layer):\n",
    "        if not (0 <= layer_config.dropout_rate < 1):\n",
    "            raise ValueError(\"dropout_rate must between 0 and 1\")\n",
    "\n",
    "        w = np.random.randn(layer_config.output_node, layer_config.input_node)\n",
    "        b = np.random.randn(layer_config.output_node, 1)\n",
    "\n",
    "        w, b = (\n",
    "            w * np.sqrt(1.0 / layer_config.input_node),\n",
    "            b * 0.01,\n",
    "        )\n",
    "        activate, activate_derivative = Tools.activate_func(layer_config.activate)\n",
    "        return cls(\n",
    "            w=w,\n",
    "            b=b,\n",
    "            dropout_rate=layer_config.dropout_rate,\n",
    "            activate_str=layer_config.activate,\n",
    "            activate=activate,\n",
    "            activate_derivative=activate_derivative,\n",
    "        )\n",
    "\n",
    "    def forward_drop_out(self, a: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        mask = np.random.binomial(1, 1 - self.dropout_rate, size=a.shape)\n",
    "        a_drop_out = (a * mask) / (1 - self.dropout_rate)\n",
    "        return a_drop_out, mask\n",
    "\n",
    "    def update_delta(\n",
    "        self,\n",
    "        delta: np.ndarray,\n",
    "        layer_next,\n",
    "        z_next: np.ndarray,\n",
    "        mask: np.ndarray = None,\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "        if mask is not None:\n",
    "            delta *= mask\n",
    "\n",
    "        return (self.w.T @ delta) * layer_next.activate_derivative(z_next)\n",
    "\n",
    "    def update(self, grad_item: BaseModelLayer, lr: float) -> None:\n",
    "        assert (\n",
    "            self.w.shape == grad_item.w.shape\n",
    "        ), f\"权重形状不匹配: {self.w.shape} vs {grad_item.w.shape}\"\n",
    "        assert self.b.shape[1] == 1\n",
    "\n",
    "        assert (\n",
    "            self.b.shape == grad_item.b.shape\n",
    "        ), f\"偏置形状不匹配: {self.b.shape} vs {grad_item.b.shape} {grad_item.w.shape}\"\n",
    "\n",
    "        self.w -= lr * grad_item.w\n",
    "        self.b -= lr * grad_item.b\n",
    "        return\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        z = self.w @ x + self.b\n",
    "        a = self.activate(z)\n",
    "        return z, a\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"w: {self.w.shape}, b: {self.b.shape}, activate: {self.activate_str}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self, layers_config: list[Layer], loss_func: str = \"sse\"):\n",
    "\n",
    "        self._input_dim, self._output_dim = (\n",
    "            layers_config[0].input_node,\n",
    "            layers_config[-1].output_node,\n",
    "        )\n",
    "\n",
    "        self._layers: dict[int, ModelLayer] = {\n",
    "            i: ModelLayer.build_layer(layer)\n",
    "            for i, layer in enumerate(layers_config, start=1)\n",
    "        }\n",
    "\n",
    "        self._layers_len = len(self._layers)\n",
    "\n",
    "        self._loss_func, self._loss_derivative = Tools.loss_func(loss_func)\n",
    "\n",
    "        # self._is_multi = self._output_dim > 1\n",
    "\n",
    "        self._model_struct = {\n",
    "            \"input_dim\": self._input_dim,\n",
    "            \"output_dim\": self._output_dim,\n",
    "            \"hidden_layers\": self._layers,\n",
    "            \"loss_func\": loss_func,\n",
    "        }\n",
    "\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def model_struct(self):\n",
    "        return self._model_struct\n",
    "\n",
    "    @property\n",
    "    def w(self):\n",
    "        return self._w\n",
    "\n",
    "    @property\n",
    "    def b(self):\n",
    "        return self._b\n",
    "\n",
    "    def forward(\n",
    "        self, x: np.ndarray, for_backward: bool = False\n",
    "    ) -> (\n",
    "        tuple[dict[int, np.ndarray], dict[int, np.ndarray], dict[int, np.ndarray]]\n",
    "        | np.ndarray\n",
    "    ):\n",
    "\n",
    "        if len(x.shape) == 1:\n",
    "            x = x[:, np.newaxis]\n",
    "\n",
    "        a_out, z_out, mask_out = {0: x}, dict(), dict()\n",
    "\n",
    "        for i, layer in self._layers.items():\n",
    "            z_out[i], a_out[i] = layer(a_out[i - 1])\n",
    "\n",
    "            if layer.dropout_rate > 0 and for_backward:\n",
    "                a_out[i], mask_out[i] = layer.forward_drop_out(a_out[i])\n",
    "\n",
    "        last_item = next(reversed(a_out.values()))\n",
    "\n",
    "        if for_backward:\n",
    "            return last_item, a_out, z_out, mask_out\n",
    "\n",
    "        return last_item\n",
    "\n",
    "    def init_delta(self, a_out: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        # if self._is_multi:\n",
    "\n",
    "        #     return self._loss_derivative(y, softmax(a_out)) / y.shape[1]\n",
    "        # print(y.shape)\n",
    "        return self._loss_derivative(y, a_out) / y.shape[1]\n",
    "\n",
    "    def backward(\n",
    "        self,\n",
    "        end_y_hat: np.ndarray,\n",
    "        a_out: dict[int, np.ndarray],\n",
    "        z_out: dict[int, np.ndarray],\n",
    "        mask_out: dict[int, np.ndarray],\n",
    "        y: np.ndarray,\n",
    "    ) -> dict[int, BaseModelLayer]:\n",
    "        # https://medium.com/@erikhallstrm/backpropagation-from-the-beginning-77356edf427d\n",
    "        \"\"\"\n",
    "        put y data is a column vector , so if data is row vector , please transpose\n",
    "        \"\"\"\n",
    "        grad = {i: BaseModelLayer.zero_like(layer) for i, layer in self._layers.items()}\n",
    "\n",
    "        # init the delta\n",
    "        delta = self.init_delta(end_y_hat, y)\n",
    "        # print({\"delta\": delta, \"y_hat\": end_y_hat, \"y\": y})\n",
    "\n",
    "        for layer_index in reversed(self._layers.keys()):\n",
    "            next_index = layer_index - 1\n",
    "\n",
    "            grad[layer_index].w = delta @ a_out[next_index].T\n",
    "\n",
    "            grad[layer_index].b = np.sum(delta, axis=1, keepdims=True)\n",
    "\n",
    "            if layer_index > 1:\n",
    "                delta = self._layers[layer_index].update_delta(\n",
    "                    delta=delta,\n",
    "                    mask=mask_out.get(layer_index, None),\n",
    "                    layer_next=self._layers[next_index],\n",
    "                    z_next=z_out[next_index],\n",
    "                )\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def _update(self, grad: dict[int, BaseModelLayer], learning_rate: float) -> None:\n",
    "        for i, grad_item in grad.items():\n",
    "            self._layers[i].update(grad_item, lr=learning_rate)\n",
    "\n",
    "        return\n",
    "\n",
    "    def __call__(self, x) -> np.ndarray:\n",
    "        return self.forward(x.T, for_backward=False)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return pformat(self._model_struct)\n",
    "\n",
    "    def _l2_regularization(self, lambda_reg: float = 0.01):\n",
    "        weight_item = [np.sum(np.square(layer.w)) for layer in self._layers.values()]\n",
    "        return lambda_reg * 0.5 * np.sum(weight_item)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        epochs: int,\n",
    "        batch_size: int,\n",
    "        learning_rate: float,\n",
    "        l2: float = 0,\n",
    "        save_folder: str = None,\n",
    "    ) -> None:\n",
    "        loss_log = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            indices = np.random.permutation(x.shape[0])\n",
    "            X_shuffled = x[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for i in range(0, X_shuffled.shape[0], batch_size):\n",
    "\n",
    "                X_batch = X_shuffled[i : i + batch_size]\n",
    "                Y_batch = y_shuffled[i : i + batch_size]\n",
    "\n",
    "                x_batch = X_batch.T\n",
    "                y_batch = Y_batch.T\n",
    "\n",
    "                y_hat, a_out, z_out, mask_out = self.forward(x_batch, for_backward=True)\n",
    "\n",
    "                loss = self._loss_func(y_batch, y_hat)\n",
    "                grad = self.backward(y_hat, a_out, z_out, mask_out, y_batch)\n",
    "\n",
    "                if l2:\n",
    "                    loss += self._l2_regularization(l2)\n",
    "\n",
    "                    # update grad with l2\n",
    "                    for i in grad.keys():\n",
    "                        grad[i].w += l2 * self._layers[i].w\n",
    "\n",
    "                # update\n",
    "                self._update(grad, learning_rate)\n",
    "\n",
    "                loss_log.append({\"epoch\": epoch, \"loss\": loss})\n",
    "                logger.info(f\"epoch {epoch}, loss {loss}\")\n",
    "\n",
    "        if save_folder is not None:\n",
    "            path = Path(save_folder)\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            df_log = pd.DataFrame(loss_log)\n",
    "            df_log.to_csv(path.joinpath(\"loss.csv\"), index=False)\n",
    "\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(df_log.index, df_log[\"loss\"], marker=\"o\", label=\"Loss\")\n",
    "\n",
    "            # Labeling the plot\n",
    "            plt.xlabel(\"Index\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.title(\"Loss vs. Index\")\n",
    "            plt.legend()\n",
    "            plt.savefig(path.joinpath(\"loss.png\"))\n",
    "\n",
    "            logger.info(f\"save to {path}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    @classmethod\n",
    "    def lazy_build(\n",
    "        cls,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        calculate_layers: int,\n",
    "        activate_func: str,\n",
    "        up_dim: int,\n",
    "        loss_func: str = \"sse\",\n",
    "    ):\n",
    "\n",
    "        lazy_config = (\n",
    "            [Layer(input_node=input_dim, output_node=up_dim, activate=activate_func)]\n",
    "            + [\n",
    "                Layer(input_node=up_dim, output_node=up_dim, activate=activate_func)\n",
    "                for _ in range(calculate_layers - 2)\n",
    "            ]\n",
    "            + [Layer(input_node=up_dim, output_node=output_dim, activate=activate_func)]\n",
    "        )\n",
    "\n",
    "        return cls(lazy_config, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fnn:\n",
    "    def __init__(self, weight_path:str):\n",
    "        self.weights = np.load(weight_path, allow_pickle=True)\n",
    "        \n",
    "        self.model = {\n",
    "            key : self.weights.item()[key] \n",
    "            for key in self.weights.item().keys()\n",
    "        }\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Fnn('./weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': array([[-0.8091491 , -0.97136003,  0.87402759, ..., -0.07917783,\n",
       "         -0.48049972,  0.14332123],\n",
       "        [ 0.13680957, -0.42658406, -0.1065486 , ..., -0.35674324,\n",
       "          0.73503393,  0.97974145],\n",
       "        [-0.98785399,  0.13864091, -0.12334028, ...,  0.97998961,\n",
       "          0.81049359, -0.96672978],\n",
       "        ...,\n",
       "        [ 0.07590487, -0.72909673, -0.49320916, ..., -0.97016631,\n",
       "          0.37905837, -0.39720247],\n",
       "        [ 0.80353272,  0.49302694,  0.78355299, ...,  0.49187097,\n",
       "          0.22976861, -0.14471415],\n",
       "        [-0.65034227, -0.44644287,  0.96614131, ..., -0.41951292,\n",
       "         -0.39306069, -0.21858973]], shape=(784, 2048)),\n",
       " 'b1': array([[ 0.92383328],\n",
       "        [-0.60657822],\n",
       "        [-0.03953955],\n",
       "        ...,\n",
       "        [-0.7922291 ],\n",
       "        [-0.06904414],\n",
       "        [-0.77732582]], shape=(2048, 1)),\n",
       " 'w2': array([[ 0.83440492,  0.73153573,  0.71223082, ..., -0.73398912,\n",
       "          0.57465462,  0.54825252],\n",
       "        [ 0.79550778,  0.74281046, -0.50577499, ..., -0.13199838,\n",
       "          0.01564983,  0.12175966],\n",
       "        [-0.48820003,  0.20851807,  0.42523802, ..., -0.36286099,\n",
       "          0.5936367 , -0.66947755],\n",
       "        ...,\n",
       "        [ 0.66793852, -0.40800141, -0.24093169, ...,  0.28226774,\n",
       "          0.6930728 ,  0.75191581],\n",
       "        [ 0.51794445, -0.39689378, -0.65465509, ...,  0.63169196,\n",
       "         -0.25076563,  0.99869845],\n",
       "        [-0.06690978,  0.09100703,  0.62792174, ..., -0.35599482,\n",
       "          0.13575622, -0.59609972]], shape=(2048, 512)),\n",
       " 'b2': array([[-0.56128292],\n",
       "        [ 0.09278767],\n",
       "        [-0.59671597],\n",
       "        [-0.64049403],\n",
       "        [-0.6305827 ],\n",
       "        [-0.17961825],\n",
       "        [ 0.15891417],\n",
       "        [-0.48095996],\n",
       "        [-0.51866201],\n",
       "        [ 0.50021052],\n",
       "        [-0.15499466],\n",
       "        [-0.14949141],\n",
       "        [ 0.3206261 ],\n",
       "        [ 0.09603298],\n",
       "        [ 0.86049476],\n",
       "        [ 0.69806293],\n",
       "        [-0.57108126],\n",
       "        [ 0.20616269],\n",
       "        [-0.33450866],\n",
       "        [ 0.63872348],\n",
       "        [ 0.40504363],\n",
       "        [-0.72603414],\n",
       "        [ 0.53075806],\n",
       "        [ 0.65222272],\n",
       "        [-0.33041568],\n",
       "        [ 0.88102051],\n",
       "        [-0.04120503],\n",
       "        [-0.54592491],\n",
       "        [-0.62018964],\n",
       "        [ 0.01797819],\n",
       "        [ 0.90179742],\n",
       "        [-0.4902922 ],\n",
       "        [ 0.46174354],\n",
       "        [-0.37548912],\n",
       "        [ 0.54146658],\n",
       "        [-0.52388836],\n",
       "        [ 0.51399913],\n",
       "        [ 0.73774099],\n",
       "        [-0.98296596],\n",
       "        [ 0.66032343],\n",
       "        [ 0.10387416],\n",
       "        [-0.43892987],\n",
       "        [-0.22823689],\n",
       "        [ 0.97912091],\n",
       "        [-0.4681245 ],\n",
       "        [ 0.42001604],\n",
       "        [ 0.15025498],\n",
       "        [ 0.06763673],\n",
       "        [ 0.66919749],\n",
       "        [ 0.82208979],\n",
       "        [-0.37479841],\n",
       "        [ 0.26957016],\n",
       "        [-0.44415238],\n",
       "        [-0.74217266],\n",
       "        [-0.05232794],\n",
       "        [-0.12730661],\n",
       "        [-0.43835307],\n",
       "        [-0.72201163],\n",
       "        [ 0.64607576],\n",
       "        [ 0.22714236],\n",
       "        [-0.63931633],\n",
       "        [ 0.8804468 ],\n",
       "        [-0.42369715],\n",
       "        [ 0.68769536],\n",
       "        [-0.57371483],\n",
       "        [-0.84075122],\n",
       "        [ 0.81936702],\n",
       "        [-0.49484475],\n",
       "        [ 0.50952931],\n",
       "        [-0.08314292],\n",
       "        [-0.12003576],\n",
       "        [-0.92910896],\n",
       "        [-0.68626297],\n",
       "        [ 0.6994866 ],\n",
       "        [ 0.52066165],\n",
       "        [-0.5481653 ],\n",
       "        [ 0.74814979],\n",
       "        [ 0.7785207 ],\n",
       "        [ 0.28640827],\n",
       "        [-0.18517364],\n",
       "        [ 0.66494409],\n",
       "        [ 0.16732695],\n",
       "        [ 0.49242127],\n",
       "        [ 0.22032401],\n",
       "        [ 0.00808663],\n",
       "        [ 0.0310829 ],\n",
       "        [ 0.28842871],\n",
       "        [-0.68631549],\n",
       "        [-0.39795629],\n",
       "        [-0.58182075],\n",
       "        [-0.25108062],\n",
       "        [ 0.27056385],\n",
       "        [ 0.98243846],\n",
       "        [ 0.04558551],\n",
       "        [ 0.3440186 ],\n",
       "        [-0.22118285],\n",
       "        [-0.06821524],\n",
       "        [ 0.8891133 ],\n",
       "        [-0.2594882 ],\n",
       "        [ 0.20864931],\n",
       "        [-0.73408983],\n",
       "        [ 0.20573059],\n",
       "        [ 0.52889372],\n",
       "        [ 0.87119239],\n",
       "        [ 0.67834277],\n",
       "        [-0.573287  ],\n",
       "        [ 0.30846369],\n",
       "        [-0.16087305],\n",
       "        [-0.05102524],\n",
       "        [-0.27847201],\n",
       "        [ 0.7684237 ],\n",
       "        [ 0.09025421],\n",
       "        [ 0.94581761],\n",
       "        [ 0.45781472],\n",
       "        [ 0.8635157 ],\n",
       "        [-0.26909178],\n",
       "        [-0.35737643],\n",
       "        [-0.93370265],\n",
       "        [-0.73007511],\n",
       "        [-0.87782792],\n",
       "        [ 0.10690307],\n",
       "        [ 0.41303592],\n",
       "        [-0.06798423],\n",
       "        [ 0.59184249],\n",
       "        [ 0.09243992],\n",
       "        [ 0.49614842],\n",
       "        [-0.33257587],\n",
       "        [ 0.13903399],\n",
       "        [ 0.214584  ],\n",
       "        [ 0.37684445],\n",
       "        [-0.67681137],\n",
       "        [ 0.00984848],\n",
       "        [-0.27554893],\n",
       "        [-0.48201946],\n",
       "        [ 0.69529425],\n",
       "        [ 0.42678591],\n",
       "        [-0.74742476],\n",
       "        [ 0.76896383],\n",
       "        [ 0.57427855],\n",
       "        [ 0.36930343],\n",
       "        [ 0.46862242],\n",
       "        [-0.61064188],\n",
       "        [ 0.339391  ],\n",
       "        [-0.20349895],\n",
       "        [-0.59776917],\n",
       "        [-0.88778115],\n",
       "        [-0.22281159],\n",
       "        [ 0.67520111],\n",
       "        [-0.74032079],\n",
       "        [-0.70679979],\n",
       "        [-0.62542947],\n",
       "        [ 0.56890425],\n",
       "        [ 0.56401316],\n",
       "        [ 0.54709416],\n",
       "        [ 0.24312559],\n",
       "        [ 0.37649136],\n",
       "        [ 0.53832018],\n",
       "        [ 0.28659355],\n",
       "        [ 0.05052575],\n",
       "        [ 0.40974447],\n",
       "        [ 0.0598647 ],\n",
       "        [-0.12664484],\n",
       "        [ 0.09557317],\n",
       "        [-0.77723692],\n",
       "        [-0.11455933],\n",
       "        [-0.22167143],\n",
       "        [ 0.52819757],\n",
       "        [-0.14231373],\n",
       "        [ 0.44500469],\n",
       "        [ 0.01707029],\n",
       "        [ 0.05374571],\n",
       "        [-0.85294736],\n",
       "        [ 0.33519962],\n",
       "        [ 0.98610451],\n",
       "        [-0.53959164],\n",
       "        [ 0.90045838],\n",
       "        [-0.56541481],\n",
       "        [-0.16264488],\n",
       "        [ 0.23984893],\n",
       "        [ 0.51241321],\n",
       "        [ 0.38847743],\n",
       "        [ 0.96379107],\n",
       "        [-0.69337155],\n",
       "        [-0.80646049],\n",
       "        [-0.47556905],\n",
       "        [-0.65096986],\n",
       "        [-0.69471918],\n",
       "        [ 0.43760379],\n",
       "        [ 0.8259935 ],\n",
       "        [-0.22676472],\n",
       "        [-0.26772277],\n",
       "        [-0.2620178 ],\n",
       "        [ 0.01315523],\n",
       "        [ 0.5955118 ],\n",
       "        [ 0.62242337],\n",
       "        [ 0.94318483],\n",
       "        [ 0.48790828],\n",
       "        [-0.02800427],\n",
       "        [-0.13815245],\n",
       "        [-0.89992963],\n",
       "        [ 0.76352327],\n",
       "        [-0.06998185],\n",
       "        [ 0.21865216],\n",
       "        [-0.95157651],\n",
       "        [-0.31630048],\n",
       "        [-0.73445809],\n",
       "        [ 0.40783984],\n",
       "        [-0.98825847],\n",
       "        [-0.57685655],\n",
       "        [-0.68673755],\n",
       "        [ 0.05235155],\n",
       "        [-0.10387786],\n",
       "        [ 0.09267609],\n",
       "        [-0.66658304],\n",
       "        [ 0.41170261],\n",
       "        [ 0.61552202],\n",
       "        [ 0.70552354],\n",
       "        [ 0.03597873],\n",
       "        [ 0.05555923],\n",
       "        [-0.90335785],\n",
       "        [-0.23582164],\n",
       "        [ 0.34156433],\n",
       "        [-0.69939431],\n",
       "        [-0.63655102],\n",
       "        [ 0.94956883],\n",
       "        [-0.72805975],\n",
       "        [-0.39461711],\n",
       "        [ 0.21401957],\n",
       "        [-0.45457462],\n",
       "        [-0.56744966],\n",
       "        [ 0.68115543],\n",
       "        [ 0.08495175],\n",
       "        [ 0.39980719],\n",
       "        [-0.2074235 ],\n",
       "        [ 0.96128199],\n",
       "        [ 0.31786753],\n",
       "        [ 0.27938007],\n",
       "        [ 0.67864743],\n",
       "        [ 0.34439489],\n",
       "        [-0.30541164],\n",
       "        [-0.49273212],\n",
       "        [ 0.22008219],\n",
       "        [-0.69449003],\n",
       "        [-0.45360426],\n",
       "        [-0.96140063],\n",
       "        [-0.31643889],\n",
       "        [ 0.9798937 ],\n",
       "        [-0.61538971],\n",
       "        [ 0.74088026],\n",
       "        [ 0.27478766],\n",
       "        [-0.75528954],\n",
       "        [ 0.37957235],\n",
       "        [ 0.41622126],\n",
       "        [-0.57449187],\n",
       "        [-0.80118337],\n",
       "        [ 0.22211238],\n",
       "        [-0.89335279],\n",
       "        [-0.62028616],\n",
       "        [ 0.57551129],\n",
       "        [-0.26715213],\n",
       "        [ 0.23177118],\n",
       "        [ 0.76806707],\n",
       "        [ 0.89688981],\n",
       "        [ 0.35266188],\n",
       "        [ 0.18439657],\n",
       "        [ 0.66996932],\n",
       "        [-0.13790863],\n",
       "        [-0.16527073],\n",
       "        [ 0.05531932],\n",
       "        [-0.45806266],\n",
       "        [ 0.22062976],\n",
       "        [ 0.68397333],\n",
       "        [ 0.62616145],\n",
       "        [ 0.80618509],\n",
       "        [-0.74917826],\n",
       "        [ 0.80419304],\n",
       "        [-0.99483403],\n",
       "        [ 0.57124115],\n",
       "        [-0.99500805],\n",
       "        [ 0.38143442],\n",
       "        [-0.19331315],\n",
       "        [-0.93451238],\n",
       "        [-0.91388737],\n",
       "        [-0.73898439],\n",
       "        [ 0.50532073],\n",
       "        [ 0.40477027],\n",
       "        [ 0.55965298],\n",
       "        [-0.50864955],\n",
       "        [ 0.51881426],\n",
       "        [ 0.40352994],\n",
       "        [-0.09298788],\n",
       "        [-0.71960669],\n",
       "        [-0.3091022 ],\n",
       "        [ 0.2179983 ],\n",
       "        [ 0.7725768 ],\n",
       "        [-0.401813  ],\n",
       "        [-0.30917219],\n",
       "        [ 0.4290907 ],\n",
       "        [ 0.14295866],\n",
       "        [-0.4803969 ],\n",
       "        [ 0.10084799],\n",
       "        [ 0.02944923],\n",
       "        [ 0.15629946],\n",
       "        [ 0.34818698],\n",
       "        [ 0.41694347],\n",
       "        [-0.2468732 ],\n",
       "        [ 0.8553403 ],\n",
       "        [ 0.02202621],\n",
       "        [-0.6020887 ],\n",
       "        [ 0.98436524],\n",
       "        [ 0.12679672],\n",
       "        [-0.64983456],\n",
       "        [-0.48790313],\n",
       "        [-0.04022093],\n",
       "        [ 0.40584982],\n",
       "        [ 0.76560497],\n",
       "        [ 0.66715592],\n",
       "        [-0.30333309],\n",
       "        [-0.33157506],\n",
       "        [-0.84476442],\n",
       "        [ 0.59917489],\n",
       "        [ 0.28764492],\n",
       "        [ 0.79358543],\n",
       "        [ 0.54580548],\n",
       "        [ 0.69888886],\n",
       "        [ 0.579655  ],\n",
       "        [ 0.60772992],\n",
       "        [-0.97789088],\n",
       "        [-0.75722402],\n",
       "        [ 0.4096343 ],\n",
       "        [ 0.40763525],\n",
       "        [ 0.2730553 ],\n",
       "        [-0.70770149],\n",
       "        [-0.84091185],\n",
       "        [ 0.38002379],\n",
       "        [ 0.29367257],\n",
       "        [-0.97298351],\n",
       "        [ 0.12071901],\n",
       "        [ 0.77617087],\n",
       "        [-0.59979721],\n",
       "        [ 0.5380977 ],\n",
       "        [ 0.73397698],\n",
       "        [-0.94275706],\n",
       "        [ 0.78784466],\n",
       "        [ 0.15170718],\n",
       "        [ 0.04507321],\n",
       "        [ 0.58818096],\n",
       "        [ 0.45488991],\n",
       "        [ 0.71820412],\n",
       "        [ 0.22467458],\n",
       "        [ 0.86231383],\n",
       "        [-0.1518408 ],\n",
       "        [ 0.77856156],\n",
       "        [-0.17194266],\n",
       "        [ 0.23152299],\n",
       "        [ 0.57556251],\n",
       "        [-0.10660244],\n",
       "        [-0.07461977],\n",
       "        [ 0.71674251],\n",
       "        [ 0.62554775],\n",
       "        [-0.72297815],\n",
       "        [ 0.31643768],\n",
       "        [ 0.20837944],\n",
       "        [ 0.66783765],\n",
       "        [-0.03526971],\n",
       "        [-0.13383958],\n",
       "        [-0.88537481],\n",
       "        [ 0.84879199],\n",
       "        [ 0.2653177 ],\n",
       "        [ 0.70869487],\n",
       "        [-0.69547245],\n",
       "        [ 0.09140375],\n",
       "        [ 0.01457743],\n",
       "        [ 0.21444318],\n",
       "        [ 0.79581116],\n",
       "        [-0.65980247],\n",
       "        [-0.28429611],\n",
       "        [-0.39117522],\n",
       "        [ 0.85579642],\n",
       "        [ 0.18793602],\n",
       "        [-0.20592818],\n",
       "        [-0.57881348],\n",
       "        [-0.50732511],\n",
       "        [ 0.05828687],\n",
       "        [-0.87536509],\n",
       "        [-0.94405192],\n",
       "        [ 0.09164066],\n",
       "        [ 0.87267628],\n",
       "        [ 0.50404228],\n",
       "        [-0.19652158],\n",
       "        [ 0.78027403],\n",
       "        [-0.971225  ],\n",
       "        [-0.76887257],\n",
       "        [ 0.82928975],\n",
       "        [ 0.90702088],\n",
       "        [ 0.74912314],\n",
       "        [ 0.44832322],\n",
       "        [-0.04610149],\n",
       "        [-0.82305482],\n",
       "        [ 0.66915175],\n",
       "        [ 0.91253573],\n",
       "        [-0.25571405],\n",
       "        [-0.2235976 ],\n",
       "        [ 0.25853972],\n",
       "        [-0.0676144 ],\n",
       "        [ 0.85547665],\n",
       "        [-0.7957841 ],\n",
       "        [ 0.31503245],\n",
       "        [ 0.82005124],\n",
       "        [ 0.75631204],\n",
       "        [ 0.63428171],\n",
       "        [ 0.89836034],\n",
       "        [-0.80151583],\n",
       "        [ 0.11713352],\n",
       "        [-0.36253575],\n",
       "        [-0.42613623],\n",
       "        [-0.61408726],\n",
       "        [-0.63591768],\n",
       "        [-0.82601274],\n",
       "        [-0.93314778],\n",
       "        [-0.37985805],\n",
       "        [-0.71756142],\n",
       "        [ 0.57138199],\n",
       "        [-0.15491993],\n",
       "        [ 0.78747536],\n",
       "        [-0.91697405],\n",
       "        [-0.87170903],\n",
       "        [-0.28047097],\n",
       "        [-0.95417737],\n",
       "        [-0.1853981 ],\n",
       "        [ 0.74895284],\n",
       "        [-0.38070584],\n",
       "        [ 0.65410741],\n",
       "        [ 0.62527157],\n",
       "        [ 0.74396096],\n",
       "        [-0.27385381],\n",
       "        [ 0.87774965],\n",
       "        [-0.02926695],\n",
       "        [-0.83597439],\n",
       "        [-0.01786198],\n",
       "        [ 0.37711496],\n",
       "        [ 0.95728709],\n",
       "        [ 0.08034904],\n",
       "        [ 0.2705276 ],\n",
       "        [ 0.03973711],\n",
       "        [-0.86404571],\n",
       "        [-0.21327746],\n",
       "        [ 0.05965886],\n",
       "        [-0.40275619],\n",
       "        [ 0.01786442],\n",
       "        [ 0.98595758],\n",
       "        [ 0.18958232],\n",
       "        [-0.20582445],\n",
       "        [-0.06812321],\n",
       "        [-0.50743593],\n",
       "        [-0.2740241 ],\n",
       "        [ 0.26522125],\n",
       "        [-0.54426083],\n",
       "        [ 0.40412054],\n",
       "        [ 0.21517486],\n",
       "        [-0.44695595],\n",
       "        [-0.95258225],\n",
       "        [ 0.7209031 ],\n",
       "        [-0.54706857],\n",
       "        [-0.89639825],\n",
       "        [-0.73149576],\n",
       "        [ 0.80992622],\n",
       "        [-0.14417952],\n",
       "        [-0.24572995],\n",
       "        [ 0.23582   ],\n",
       "        [ 0.06406346],\n",
       "        [ 0.38176303],\n",
       "        [ 0.71680423],\n",
       "        [ 0.84942466],\n",
       "        [-0.9490257 ],\n",
       "        [ 0.00197704],\n",
       "        [-0.55460808],\n",
       "        [ 0.06838654],\n",
       "        [ 0.63811657],\n",
       "        [-0.68688457],\n",
       "        [ 0.21335537],\n",
       "        [ 0.84969891],\n",
       "        [-0.54862293],\n",
       "        [-0.5490856 ],\n",
       "        [ 0.84614876],\n",
       "        [-0.83464601],\n",
       "        [-0.2873742 ],\n",
       "        [ 0.92768995],\n",
       "        [ 0.48240665],\n",
       "        [-0.02704127],\n",
       "        [-0.43123819],\n",
       "        [ 0.52139377],\n",
       "        [-0.38002739],\n",
       "        [-0.77913595],\n",
       "        [-0.58662119],\n",
       "        [-0.7346477 ],\n",
       "        [-0.00301414],\n",
       "        [-0.00107637],\n",
       "        [-0.92942584],\n",
       "        [-0.44683623],\n",
       "        [ 0.52888897],\n",
       "        [-0.68223079],\n",
       "        [-0.32140851],\n",
       "        [ 0.82334191],\n",
       "        [-0.057503  ],\n",
       "        [-0.73624699],\n",
       "        [ 0.61384879],\n",
       "        [ 0.49734102],\n",
       "        [ 0.24141321],\n",
       "        [-0.63430461],\n",
       "        [-0.9546444 ],\n",
       "        [-0.81499169]]),\n",
       " 'w3': array([[ 0.0990551 , -0.38645488, -0.83952859, ..., -0.45456102,\n",
       "         -0.48669601,  0.81982247],\n",
       "        [-0.93673632, -0.86004555,  0.59102602, ..., -0.01502977,\n",
       "          0.05403967, -0.59957093],\n",
       "        [-0.66837887, -0.64898481, -0.36962526, ...,  0.56119612,\n",
       "         -0.50613865,  0.59232446],\n",
       "        ...,\n",
       "        [-0.88794651,  0.12539028, -0.95238651, ..., -0.46998565,\n",
       "          0.83370111,  0.51623999],\n",
       "        [ 0.18476064,  0.23791652,  0.14750423, ..., -0.97322012,\n",
       "         -0.13672375,  0.87592938],\n",
       "        [ 0.37049113,  0.09125582, -0.90815961, ..., -0.86695778,\n",
       "          0.07836843, -0.55994479]], shape=(512, 10)),\n",
       " 'b3': array([[ 0.68547609],\n",
       "        [-0.21909326],\n",
       "        [-0.60462302],\n",
       "        [-0.49063362],\n",
       "        [ 0.44347103],\n",
       "        [ 0.97773235],\n",
       "        [-0.9270019 ],\n",
       "        [-0.56563634],\n",
       "        [ 0.9288921 ],\n",
       "        [ 0.67385306]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
