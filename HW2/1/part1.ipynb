{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5f2d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "# https://medium.com/willhanchen/%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86-spacy-%E5%88%9D%E6%8E%A2%E5%BC%B7%E5%A4%A7%E7%9A%84%E5%B7%A5%E5%85%B7%E5%BA%ABspacy-%E8%AE%93%E6%A9%9F%E5%99%A8%E8%AE%80%E6%87%82%E6%88%91%E5%80%91%E7%9A%84%E8%AA%9E%E8%A8%80%E5%90%A7-4a35daa895d0\n",
    "import spacy\n",
    "from spacy.cli import download\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.models import FastText, word2vec, Word2Vec\n",
    "from gensim.utils import tokenize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# for model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "import math\n",
    "from natsort import natsorted\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import random_split   \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1157b98e",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8adc429",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData:\n",
    "    # embedding model use fasttext\n",
    "    def __init__(self, train_data:pd.DataFrame, test_data:pd.DataFrame):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.embedding_model_name = None\n",
    "        \n",
    "        self.setup()\n",
    "        return \n",
    "    \n",
    "    def train_embedding_model(self, result_files:dict, model_type:str=\"skipgram\", embedding_size:int=100):\n",
    "        self.embedding_model_name = model_type\n",
    "        data_folder = Path(result_files[\"folder\"])\n",
    "        file_to_train = data_folder / result_files[\"embedding_training\"]\n",
    "        \n",
    "        sentences = word2vec.LineSentence(file_to_train)\n",
    "        \n",
    "        self.model = FastText(\n",
    "            sentences,\n",
    "            vector_size=embedding_size,\n",
    "            window=5,\n",
    "            min_count=1,\n",
    "            sg=1 if model_type == \"skipgram\" else 0, # skipgram = 1, cbow = 0\n",
    "            hs=0,\n",
    "            negative=5,\n",
    "            epochs=10,\n",
    "        )\n",
    "        \n",
    "        model_folder = Path(\"model\")\n",
    "        model_folder.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        model_file_name =  f\"{model_type}.model\"\n",
    "        self.model.save(str(model_folder / model_file_name))\n",
    "        \n",
    "        return {\"model_folder\": str(model_folder), \"model_file_name\":model_file_name, \"model_type\": model_type}\n",
    "    \n",
    "    def load_embedding_model(self, model_path:str):\n",
    "        self.embedding_model_name = model_path\n",
    "        self.model = FastText.load(model_path)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def setup(self):\n",
    "        # 第一次執行需要下載停用詞資源\n",
    "        nltk.download(\"stopwords\")\n",
    "        download(\"en_core_web_sm\")\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        return\n",
    "    \n",
    "    def clean_text_spacy(self, text:str) -> str:\n",
    "        condition = lambda token : not token.is_stop and not token.is_punct and not token.is_digit\n",
    "        \n",
    "        doc = self.nlp(text)\n",
    "        # 篩選出非停用字、非標點符號、非數字的 token，並轉為小寫\n",
    "        tokens = [token.text.lower() for token in doc if condition(token)]\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def clean_text_nltk(self, text: str) -> str:\n",
    "        # 轉成小寫\n",
    "        text = text.lower()\n",
    "        # 移除數字\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        # 移除標點符號\n",
    "        text = re.sub(r'[{}]'.format(re.escape(string.punctuation)), '', text)\n",
    "        # 移除多餘空白\n",
    "        text = text.strip()\n",
    "        # 斷詞\n",
    "        tokens = text.split()\n",
    "        # 移除停用字\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        # 回傳清理後的結果\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def batch_process_text(self, process_funcs:list[Callable], max_workers:int=None):\n",
    "        result = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [executor.submit(process_func) for process_func in process_funcs]\n",
    "            for future in as_completed(futures):\n",
    "                result.append(future.result())\n",
    "                \n",
    "        return result\n",
    "    \n",
    "    def wrapper_function(self,id_:int, type_:int, headline_in:str, short_description_in:str, label:int, process_func:str):\n",
    "        \n",
    "        def process_it():\n",
    "            headline = process_func(headline_in)\n",
    "            short_description = process_func(short_description_in)\n",
    "            item = \"<CLS> \" + headline + \" <SEP> \" +  short_description + \" <END>\"\n",
    "            return {\"id\": id_, \"type\": type_, \"process_text\": item, \"label\": label}\n",
    "        \n",
    "        return process_it\n",
    "    \n",
    "    \n",
    "    def process_all_data(self, process:str, batch_size:int=100, max_workers:int=None):\n",
    "        process_func = self.clean_text_spacy if process == \"spacy\" else self.clean_text_nltk\n",
    "        \n",
    "        train_data_df = self.train_data[[\"id\" , \"headline\", \"short_description\", \"label\"]].copy()\n",
    "        train_data_df[\"type\"] = \"train\"\n",
    "        test_data_df = self.test_data[[\"id\" ,\"headline\", \"short_description\"]].copy()\n",
    "        test_data_df[\"type\"] = \"test\"\n",
    "        test_data_df[\"label\"] = \"IDK\"\n",
    "        \n",
    "        data_df = pd.concat([train_data_df, test_data_df], axis=0)\n",
    "        \n",
    "        \n",
    "        jobs = []\n",
    "        for _, row in tqdm(data_df.iterrows(), total=data_df.shape[0], desc=\"Build Jobs\"):\n",
    "            id_ = row[\"id\"]\n",
    "            type_ = row[\"type\"]\n",
    "            headline = row[\"headline\"]\n",
    "            short_description = row[\"short_description\"]\n",
    "            label = row[\"label\"]\n",
    "            \n",
    "            # 使用 wrapper_function 來包裝 process_func\n",
    "            job = self.wrapper_function(id_, type_, headline, short_description, label , process_func)\n",
    "            jobs.append(job)\n",
    "        \n",
    "        result = []\n",
    "        for batch in tqdm(range(0, len(jobs), batch_size), desc=\"Processing text\"):\n",
    "            batch_jobs = jobs[batch:batch + batch_size]\n",
    "            out = self.batch_process_text(batch_jobs, max_workers)\n",
    "            result.extend(out)\n",
    "        \n",
    "        \n",
    "        output_df = pd.DataFrame(result)\n",
    "        \n",
    "        folder = Path(\"temp\")\n",
    "        folder.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        prefix = f\"temp_{process}\"\n",
    "        \n",
    "        # to temp file for embedding\n",
    "        with open(folder / f\"{prefix}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            # add special tokens\n",
    "            f.write(\"<PAD> <CLS> <SEP> <END> <UNK>\\n\")\n",
    "            for text in output_df[\"process_text\"]:\n",
    "                f.write(text + \"\\n\")\n",
    "        \n",
    "        # save processed data\n",
    "        output_df.to_csv(folder / f\"{prefix}.csv\", index=False)\n",
    "        \n",
    "        output_dict = {\"folder\":str(folder), \"embedding_training\" :f\"{prefix}.txt\", \"record\": f\"{prefix}.csv\" }\n",
    "        # split train test data\n",
    "        train_test_path = TextData.split_train_test_data(output_dict)\n",
    "        \n",
    "        return output_dict | train_test_path\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_train_test_data(process_all_data_output:dict):\n",
    "        \n",
    "        folder = process_all_data_output[\"folder\"]\n",
    "        folder = Path(folder) \n",
    "        record_file = process_all_data_output[\"record\"]\n",
    "        record_path = folder / record_file\n",
    "        df = pd.read_csv(record_path, low_memory=False, dtype={\"process_text\": str})\n",
    "        \n",
    "        df_train = df[df[\"type\"] == \"train\"].copy()\n",
    "        df_test = df[df[\"type\"] == \"test\"].copy()\n",
    "        \n",
    "        df_train = df_train.sort_values(by=[\"id\"]).reset_index(drop=True)\n",
    "        df_test = df_test.sort_values(by=[\"id\"]).reset_index(drop=True)\n",
    "        \n",
    "        df_train = df_train.drop(columns=[\"type\"])\n",
    "        df_test = df_test.drop(columns=[\"type\"])\n",
    "        \n",
    "        prefix = record_file.replace(\".csv\", \"\")    \n",
    "        \n",
    "        train_file = folder / f\"{prefix}_train.csv\"\n",
    "        test_file = folder / f\"{prefix}_test.csv\"\n",
    "        \n",
    "        df_train.to_csv(train_file, index=False)\n",
    "        df_test.to_csv(test_file, index=False)\n",
    "        return {\"train_path\": str(train_file), \"test_path\": str(test_file)}\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def load_from_folder(folder:str): \n",
    "        path = Path(folder)\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Folder {folder} does not exist.\")\n",
    "        \n",
    "        train_data_json = path / \"News_train.json\"\n",
    "        test_data_json = path / \"News_test.json\"\n",
    "        \n",
    "        with open(train_data_json, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            train_data = [json.loads(line) for line in lines]   \n",
    "            \n",
    "        with open(test_data_json, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            test_data = [json.loads(line) for line in lines]\n",
    "        \n",
    "        train_data = pd.DataFrame(train_data)\n",
    "        test_data = pd.DataFrame(test_data)\n",
    "        \n",
    "        return TextData(train_data, test_data)\n",
    "    \n",
    "    \n",
    "    def tokenize(self, text:str | list[str]) -> np.ndarray:\n",
    "        # 使用 FastText 模型進行斷詞\n",
    "        unk_id = self.model.wv.key_to_index.get(\"<UNK>\", 0)\n",
    "        \n",
    "        if isinstance(text, str):\n",
    "            text_ls = text.split()\n",
    "        elif isinstance(text, list):\n",
    "            text_ls = text\n",
    "\n",
    "        token_nums = [\n",
    "            self.model.wv.key_to_index.get(text, unk_id)\n",
    "            for text in text_ls\n",
    "        ]\n",
    "        \n",
    "        return token_nums\n",
    "    \n",
    "    \n",
    "    def embedding_data(self):\n",
    "        vocab = self.model.wv.index_to_key\n",
    "        vocab_size = len(vocab)\n",
    "        embed_dim = self.model.wv.vector_size\n",
    "        embedding_weight = torch.FloatTensor(\n",
    "            np.array([self.model.wv[word] for word in vocab])\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"vocab\": vocab,\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"embed_dim\": embed_dim,\n",
    "            \"embedding_weight\": embedding_weight\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b54d3985",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"./2025-deep-learning-hw-2-text-classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bb793da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/keithlin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "text_data = TextData.load_from_folder(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da919086",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESS_TYPE = \"spacy\"\n",
    "PROCESS_DATA_BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a8cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = text_data.process_all_data(PROCESS_TYPE, batch_size=PROCESS_DATA_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9483bc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'folder': 'temp', 'embedding_training': 'temp_spacy.txt', 'record': 'temp_spacy.csv', 'train_path': 'temp_spacy_train.csv', 'test_path': 'temp_spacy_test.csv'}\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    folder = Path(\"temp\")\n",
    "    folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    prefix = f\"temp_{PROCESS_TYPE}\"\n",
    "    output = {\n",
    "        \"folder\":str(folder), \n",
    "        \"embedding_training\" :f\"{prefix}.txt\", \n",
    "        \"record\": f\"{prefix}.csv\",\n",
    "        \"train_path\":f\"{prefix}_train.csv\", \n",
    "        \"test_path\": f\"{prefix}_test.csv\" \n",
    "    }\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8e1a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_METHOD = \"skipgram\"\n",
    "EMBEDDING_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babd3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = text_data.train_embedding_model(output, model_type=EMBEDDING_METHOD, embedding_size=EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f24d4630",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    model_file = {\"model_folder\": \"model\", \"model_file_name\":f\"{EMBEDDING_METHOD}.model\", \"model_type\": EMBEDDING_METHOD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9086040",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_PATH = Path(model_file[\"model_folder\"]) / model_file[\"model_file_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6648623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data.load_embedding_model(str(EMBEDDING_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227fc6c8",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "995f6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataSet(Dataset):\n",
    "    def __init__(self, df_in:pd.DataFrame, key_to_index:dict, for_test:bool=False):\n",
    "        super().__init__()\n",
    "        self.key_to_index = key_to_index\n",
    "        self.data = df_in\n",
    "        \n",
    "        self.labels = natsorted(set(df_in[\"label\"].values))\n",
    "        self.for_test = for_test\n",
    "        return\n",
    "    \n",
    "    def detokenize(self, token_nums:list[int]) -> str:\n",
    "        # 使用 FastText 模型進行斷詞\n",
    "        index_to_key = list(self.key_to_index.keys())\n",
    "        text = \" \".join([\n",
    "            index_to_key[token_num]\n",
    "            for token_num in token_nums\n",
    "        ])\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text:str | list[str]) -> np.ndarray:\n",
    "        # 使用 FastText 模型進行斷詞\n",
    "        unk_id = self.key_to_index.get(\"<UNK>\", 0)\n",
    "        \n",
    "        if isinstance(text, str):\n",
    "            text_ls = text.split()\n",
    "        elif isinstance(text, list):\n",
    "            text_ls = text\n",
    "\n",
    "        token_nums = [\n",
    "            self.key_to_index.get(text, unk_id)\n",
    "            for text in text_ls\n",
    "        ]\n",
    "        \n",
    "        return token_nums\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index:int):\n",
    "        line_data = self.data.iloc[index]\n",
    "        text = line_data[\"process_text\"]\n",
    "        label = int(line_data[\"label\"])\n",
    "        token_nums = self.tokenize(text)\n",
    "        token_nums = torch.tensor(token_nums, dtype=torch.long)\n",
    "        \n",
    "        if self.for_test:\n",
    "            return token_nums\n",
    "        \n",
    "        return token_nums, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,     \n",
    "        text_data:TextData,\n",
    "        process_all_data_output:dict, \n",
    "        batch_size:int=32, \n",
    "        num_workers:int=0, \n",
    "        pin_memory:bool=False,\n",
    "        val_ratio:float=0.1,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.text_data = text_data  \n",
    "        self.key_to_index = text_data.model.wv.key_to_index\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.train_df_path = process_all_data_output[\"train_path\"]\n",
    "        self.test_df_path = process_all_data_output[\"test_path\"]\n",
    "        self.val_ratio = val_ratio\n",
    "        return \n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            train_df = pd.read_csv(self.train_df_path)\n",
    "            large_data = TextDataSet(train_df, key_to_index=self.key_to_index)\n",
    "            total_size = len(large_data)\n",
    "            val_size = int(total_size * self.val_ratio)\n",
    "            train_size = total_size - val_size\n",
    "            \n",
    "            self.train_dataset, self.val_dataset = random_split(\n",
    "                large_data, [train_size, val_size]\n",
    "            )\n",
    "            \n",
    "            self.predict_df = pd.read_csv(self.test_df_path)\n",
    "            self.predict_dataset = TextDataSet(self.predict_df, key_to_index=self.key_to_index, for_test=True)\n",
    "            \n",
    "        \n",
    "        elif stage == \"predict\":\n",
    "            self.predict_df = pd.read_csv(self.test_df_path)\n",
    "            self.predict_dataset = TextDataSet(self.predict_df, key_to_index=self.key_to_index, for_test=True)\n",
    "            return\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            shuffle=True,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffb47d2",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e768ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 若 d_model 為奇數，cos 會略少一個維度\n",
    "        if d_model % 2 == 1:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class EmbeddingConfig:\n",
    "    vocab_size: int\n",
    "    embed_dim: int\n",
    "\n",
    "    # fast_text_pretrained\n",
    "    state: bool = False\n",
    "    model_path: str = None\n",
    "    freeze: bool = False\n",
    "\n",
    "    @staticmethod\n",
    "    def config(vocab_size: int, embed_dim: int):\n",
    "        return EmbeddingConfig(vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_embedding_pretrain(embedding_path: str, freeze: bool = True):\n",
    "        return EmbeddingConfig(\n",
    "            vocab_size=0,\n",
    "            embed_dim=0,\n",
    "            state=True,\n",
    "            model_path=embedding_path,\n",
    "            freeze=freeze,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def embedding_data(model: Word2Vec):\n",
    "        vocab = model.wv.index_to_key\n",
    "        vocab_size = len(vocab)\n",
    "        embed_dim = model.wv.vector_size\n",
    "        embedding_weight = torch.FloatTensor(\n",
    "            np.array([model.wv[word] for word in vocab])\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"vocab\": vocab,\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"embed_dim\": embed_dim,\n",
    "            \"embedding_weight\": embedding_weight,\n",
    "        }\n",
    "\n",
    "    def build_embedding_weight(self) -> torch.Tensor:\n",
    "\n",
    "        if not self.state:\n",
    "            raise ValueError(\n",
    "                \"EmbeddingConfig: fasttext pretrained embedding not found.\"\n",
    "            )\n",
    "\n",
    "        file_path = Path(self.model_path)\n",
    "        if not file_path.exists():\n",
    "            raise ValueError(\n",
    "                f\"EmbeddingConfig: fasttext pretrained embedding not found. {file_path}\"\n",
    "            )\n",
    "\n",
    "        model = FastText.load(str(file_path))\n",
    "        embedding_data = self.embedding_data(model)\n",
    "        self.vocab_size = embedding_data[\"vocab_size\"]\n",
    "        self.embed_dim = embedding_data[\"embed_dim\"]\n",
    "\n",
    "        return embedding_data[\"embedding_weight\"]\n",
    "\n",
    "\n",
    "class TransformerClassifier(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_dict: dict,\n",
    "        num_classes: int,\n",
    "        nhead: int = 8,\n",
    "        num_layers: int = 2,\n",
    "        dim_feedforward: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "        max_seq_length: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # vocab_size, embed_dim, embedding_weight = self.decode_config(config_dict)\n",
    "        config = EmbeddingConfig(**config_dict)\n",
    "        self.embedding, config = self.build_embedding(config)\n",
    "        self.embed_dim = config.embed_dim\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(self.embed_dim, dropout, max_seq_length)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # 可採用簡單的池化方式，例如取 [CLS] token 的輸出\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, self.embed_dim//2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.embed_dim//2, num_classes),\n",
    "        )\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        return\n",
    "\n",
    "    def build_embedding(self, config: EmbeddingConfig):\n",
    "\n",
    "        if not config.state:\n",
    "            return nn.Embedding(config.vocab_size, config.embed_dim), config\n",
    "\n",
    "        embedding_weight = config.build_embedding_weight()\n",
    "\n",
    "        return (\n",
    "            nn.Embedding.from_pretrained(embedding_weight, freeze=config.freeze),\n",
    "            config,\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (batch_size, seq_len) --> token ids\n",
    "        x = self.embedding(src) * math.sqrt(\n",
    "            self.embed_dim\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        x = self.pos_encoder(x)\n",
    "        # TransformerEncoder 預設輸入 shape: (seq_len, batch_size, embed_dim)\n",
    "        # x = x.transpose(0, 1)\n",
    "        x = self.transformer_encoder(x)  # (seq_len, batch_size, embed_dim)\n",
    "        # 此處假設第一個 token 為 [CLS] token，可用於分類\n",
    "        # cls_token = x[0]  # shape: (batch_size, embed_dim)\n",
    "        # x = x[:, 0, :]  # shape: (batch_size, embed_dim)\n",
    "        x = x.mean(dim=1)  # shape: (batch_size, embed_dim)\n",
    "        logits = self.classifier(x)  # shape: (batch_size, num_classes)\n",
    "        return logits\n",
    "\n",
    "    @staticmethod\n",
    "    def load_from_embedding_pretrained(\n",
    "        embedding_path: str,\n",
    "        num_classes: int,\n",
    "        feeze_pretrained_embedding: bool = True,\n",
    "        nhead: int = 8,\n",
    "        num_layers: int = 2,\n",
    "        dim_feedforward: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "        max_seq_length: int = 512,\n",
    "    ):\n",
    "        embedding_config = EmbeddingConfig.load_embedding_pretrain(\n",
    "            embedding_path, feeze_pretrained_embedding\n",
    "        )\n",
    "\n",
    "        return TransformerClassifier(\n",
    "            asdict(embedding_config),\n",
    "            num_classes=num_classes,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            max_seq_length=max_seq_length,\n",
    "        )\n",
    "    @staticmethod\n",
    "    def build_model(\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        num_classes: int,\n",
    "        nhead: int = 8,\n",
    "        num_layers: int = 2,\n",
    "        dim_feedforward: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "        max_seq_length: int = 512,\n",
    "    ):\n",
    "        embedding_config = EmbeddingConfig.config(\n",
    "            vocab_size=vocab_size, embed_dim=embed_dim\n",
    "        )\n",
    "        return TransformerClassifier(\n",
    "            asdict(embedding_config),\n",
    "            num_classes=num_classes,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            max_seq_length=max_seq_length,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ade99",
   "metadata": {},
   "source": [
    "## Build Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "993cb845",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerClassifier.load_from_embedding_pretrained(\n",
    "    str(EMBEDDING_MODEL_PATH),\n",
    "    num_classes=4,\n",
    "    feeze_pretrained_embedding=True,\n",
    "    nhead=8,\n",
    "    num_layers=2,\n",
    "    dim_feedforward=512,\n",
    "    dropout=0.1,\n",
    "    max_seq_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2f2461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerClassifier(\n",
      "  (embedding): Embedding(74602, 128)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52a9ea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randint(0, 100, (2, 512))\n",
    "print(dummy_input.shape)\n",
    "dummy_output = model(dummy_input)\n",
    "print(dummy_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "72474e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # 範例：假設詞彙大小 10000, 嵌入維度 128, 分類類別數 5, 輸入長度 50\n",
    "# batch_size = 16\n",
    "# seq_len = 50\n",
    "# vocab_size = 10000\n",
    "# embed_dim = 128\n",
    "# num_classes = 5\n",
    "\n",
    "# model = TransformerClassifier(vocab_size, embed_dim, num_classes)\n",
    "# sample_input = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "# output = model(sample_input)\n",
    "# print(output.shape)  # 預期：(batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = 10000\n",
    "# embed_dim = 128\n",
    "# model = TransformerClassifier.build_model(\n",
    "#     vocab_size=vocab_size,\n",
    "#     embed_dim=embed_dim,\n",
    "#     num_classes=5,\n",
    "#     nhead=8,\n",
    "#     num_layers=6,\n",
    "#     dim_feedforward=512,\n",
    "#     dropout=0.1,\n",
    "#     max_seq_length=512,\n",
    "# )\n",
    "# print(model)\n",
    "# dummy_input = torch.randint(0, vocab_size, (16, 50))\n",
    "# output = model(dummy_input)\n",
    "# print(output.shape)  # 預期：(batch_size, num_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
