{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "# https://medium.com/willhanchen/%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86-spacy-%E5%88%9D%E6%8E%A2%E5%BC%B7%E5%A4%A7%E7%9A%84%E5%B7%A5%E5%85%B7%E5%BA%ABspacy-%E8%AE%93%E6%A9%9F%E5%99%A8%E8%AE%80%E6%87%82%E6%88%91%E5%80%91%E7%9A%84%E8%AA%9E%E8%A8%80%E5%90%A7-4a35daa895d0\n",
    "import spacy\n",
    "from spacy.cli import download\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.models import FastText, word2vec, Word2Vec\n",
    "from gensim.utils import tokenize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# for model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "import math\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1157b98e",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8adc429",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData:\n",
    "    # embedding model use fasttext\n",
    "    def __init__(self, train_data:pd.DataFrame, test_data:pd.DataFrame):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.embedding_model_name = None\n",
    "        \n",
    "        self.setup()\n",
    "        return \n",
    "    \n",
    "    def train_embedding_model(self, result_files:dict, model_type:str=\"skipgram\", embedding_size:int=100):\n",
    "        self.embedding_model_name = model_type\n",
    "        data_folder = Path(result_files[\"folder\"])\n",
    "        file_to_train = data_folder / result_files[\"embedding_training\"]\n",
    "        \n",
    "        sentences = word2vec.LineSentence(file_to_train)\n",
    "        \n",
    "        self.model = FastText(\n",
    "            sentences,\n",
    "            vector_size=embedding_size,\n",
    "            window=5,\n",
    "            min_count=1,\n",
    "            sg=1 if model_type == \"skipgram\" else 0, # skipgram = 1, cbow = 0\n",
    "            hs=0,\n",
    "            negative=5,\n",
    "            epochs=10,\n",
    "        )\n",
    "        \n",
    "        model_folder = Path(\"model\")\n",
    "        model_folder.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        model_file_name =  f\"{model_type}.model\"\n",
    "        self.model.save(str(model_folder / model_file_name))\n",
    "        \n",
    "        return {\"model_folder\": str(model_folder), \"model_file_name\":model_file_name, \"model_type\": model_type}\n",
    "    \n",
    "    def load_embedding_model(self, model_path:str):\n",
    "        self.embedding_model_name = model_path\n",
    "        self.model = FastText.load(model_path)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def setup(self):\n",
    "        # 第一次執行需要下載停用詞資源\n",
    "        nltk.download(\"stopwords\")\n",
    "        download(\"en_core_web_sm\")\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        return\n",
    "    \n",
    "    def clean_text_spacy(self, text:str) -> str:\n",
    "        condition = lambda token : not token.is_stop and not token.is_punct and not token.is_digit\n",
    "        \n",
    "        doc = self.nlp(text)\n",
    "        # 篩選出非停用字、非標點符號、非數字的 token，並轉為小寫\n",
    "        tokens = [token.text.lower() for token in doc if condition(token)]\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def clean_text_nltk(self, text: str) -> str:\n",
    "        # 轉成小寫\n",
    "        text = text.lower()\n",
    "        # 移除數字\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        # 移除標點符號\n",
    "        text = re.sub(r'[{}]'.format(re.escape(string.punctuation)), '', text)\n",
    "        # 移除多餘空白\n",
    "        text = text.strip()\n",
    "        # 斷詞\n",
    "        tokens = text.split()\n",
    "        # 移除停用字\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        # 回傳清理後的結果\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def batch_process_text(self, process_funcs:list[Callable], max_workers:int=None):\n",
    "        result = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [executor.submit(process_func) for process_func in process_funcs]\n",
    "            for future in as_completed(futures):\n",
    "                result.append(future.result())\n",
    "                \n",
    "        return result\n",
    "    \n",
    "    def wrapper_function(self,id_:int, type_:int, headline_in:str, short_description_in:str, label:int, process_func:str):\n",
    "        \n",
    "        def process_it():\n",
    "            headline = process_func(headline_in)\n",
    "            short_description = process_func(short_description_in)\n",
    "            item = \"<CLS> \" + headline + \" <SEP> \" +  short_description + \" <END>\"\n",
    "            return {\"id\": id_, \"type\": type_, \"process_text\": item, \"label\": label}\n",
    "        \n",
    "        return process_it\n",
    "    \n",
    "    \n",
    "    def process_all_data(self, process:str, batch_size:int=100, max_workers:int=None):\n",
    "        process_func = self.clean_text_spacy if process == \"spacy\" else self.clean_text_nltk\n",
    "        \n",
    "        train_data_df = self.train_data[[\"id\" , \"headline\", \"short_description\", \"label\"]].copy()\n",
    "        train_data_df[\"type\"] = \"train\"\n",
    "        test_data_df = self.test_data[[\"id\" ,\"headline\", \"short_description\"]].copy()\n",
    "        test_data_df[\"type\"] = \"test\"\n",
    "        test_data_df[\"label\"] = \"IDK\"\n",
    "        \n",
    "        data_df = pd.concat([train_data_df, test_data_df], axis=0)\n",
    "        \n",
    "        \n",
    "        jobs = []\n",
    "        for _, row in tqdm(data_df.iterrows(), total=data_df.shape[0], desc=\"Build Jobs\"):\n",
    "            id_ = row[\"id\"]\n",
    "            type_ = row[\"type\"]\n",
    "            headline = row[\"headline\"]\n",
    "            short_description = row[\"short_description\"]\n",
    "            label = row[\"label\"]\n",
    "            \n",
    "            # 使用 wrapper_function 來包裝 process_func\n",
    "            job = self.wrapper_function(id_, type_, headline, short_description, label , process_func)\n",
    "            jobs.append(job)\n",
    "        \n",
    "        result = []\n",
    "        for batch in tqdm(range(0, len(jobs), batch_size), desc=\"Processing text\"):\n",
    "            batch_jobs = jobs[batch:batch + batch_size]\n",
    "            out = self.batch_process_text(batch_jobs, max_workers)\n",
    "            result.extend(out)\n",
    "        \n",
    "        \n",
    "        output_df = pd.DataFrame(result)\n",
    "        \n",
    "        folder = Path(\"temp\")\n",
    "        folder.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        prefix = f\"temp_{process}\"\n",
    "        \n",
    "        # to temp file for embedding\n",
    "        with open(folder / f\"{prefix}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            # add special tokens\n",
    "            f.write(\"<PAD> <CLS> <SEP> <END> <UNK>\\n\")\n",
    "            for text in output_df[\"process_text\"]:\n",
    "                f.write(text + \"\\n\")\n",
    "        \n",
    "        # save processed data\n",
    "        output_df.to_csv(folder / f\"{prefix}.csv\", index=False)\n",
    "        \n",
    "        return {\"folder\":str(folder), \"embedding_training\" :f\"{prefix}.txt\", \"record\": f\"{prefix}.csv\" }\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_from_folder(folder:str): \n",
    "        path = Path(folder)\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Folder {folder} does not exist.\")\n",
    "        \n",
    "        train_data_json = path / \"News_train.json\"\n",
    "        test_data_json = path / \"News_test.json\"\n",
    "        \n",
    "        with open(train_data_json, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            train_data = [json.loads(line) for line in lines]   \n",
    "            \n",
    "        with open(test_data_json, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            test_data = [json.loads(line) for line in lines]\n",
    "        \n",
    "        train_data = pd.DataFrame(train_data)\n",
    "        test_data = pd.DataFrame(test_data)\n",
    "        \n",
    "        return TextData(train_data, test_data)\n",
    "    \n",
    "    \n",
    "    def tokenize(self, text:str | list[str]) -> np.ndarray:\n",
    "        # 使用 FastText 模型進行斷詞\n",
    "        unk_id = self.model.wv.key_to_index.get(\"<UNK>\", 0)\n",
    "        \n",
    "        if isinstance(text, str):\n",
    "            text_ls = text.split()\n",
    "        elif isinstance(text, list):\n",
    "            text_ls = text\n",
    "\n",
    "        token_nums = [\n",
    "            self.model.wv.key_to_index.get(text, unk_id)\n",
    "            for text in text_ls\n",
    "        ]\n",
    "        \n",
    "        return token_nums\n",
    "    \n",
    "    \n",
    "    def embedding_data(self):\n",
    "        vocab = self.model.wv.index_to_key\n",
    "        vocab_size = len(vocab)\n",
    "        embed_dim = self.model.wv.vector_size\n",
    "        embedding_weight = torch.FloatTensor(\n",
    "            np.array([self.model.wv[word] for word in vocab])\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"vocab\": vocab,\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"embed_dim\": embed_dim,\n",
    "            \"embedding_weight\": embedding_weight\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bb793da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/keithlin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "text_data = TextData.load_from_folder(\"./2025-deep-learning-hw-2-text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a8cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = text_data.process_all_data(\"spacy\", batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9483bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = Path(\"temp\")\n",
    "folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "prefix = f\"temp_spacy\"\n",
    "output = {\"folder\":str(folder), \"embedding_training\" :f\"{prefix}.txt\", \"record\": f\"{prefix}.csv\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "babd3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = text_data.train_embedding_model(output, model_type=\"skipgram\", embedding_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f24d4630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_folder': 'model',\n",
       " 'model_file_name': 'skipgram.model',\n",
       " 'model_type': 'skipgram'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffb47d2",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e768ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 若 d_model 為奇數，cos 會略少一個維度\n",
    "        if d_model % 2 == 1:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class EmbeddingConfig:\n",
    "    vocab_size: int\n",
    "    embed_dim: int\n",
    "\n",
    "    # fast_text_pretrained\n",
    "    state: bool = False\n",
    "    model_path: str = None\n",
    "    freeze: bool = False\n",
    "\n",
    "    @staticmethod\n",
    "    def config(vocab_size: int, embed_dim: int):\n",
    "        return EmbeddingConfig(vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_embedding_pretrain(embedding_path: str, freeze: bool = True):\n",
    "        return EmbeddingConfig(\n",
    "            vocab_size=0,\n",
    "            embed_dim=0,\n",
    "            state=True,\n",
    "            model_path=embedding_path,\n",
    "            freeze=freeze,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def embedding_data(model: Word2Vec):\n",
    "        vocab = model.wv.index_to_key\n",
    "        vocab_size = len(vocab)\n",
    "        embed_dim = model.wv.vector_size\n",
    "        embedding_weight = torch.FloatTensor(\n",
    "            np.array([model.wv[word] for word in vocab])\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"vocab\": vocab,\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"embed_dim\": embed_dim,\n",
    "            \"embedding_weight\": embedding_weight,\n",
    "        }\n",
    "\n",
    "    def build_embedding_weight(self) -> torch.Tensor:\n",
    "\n",
    "        if not self.state:\n",
    "            raise ValueError(\n",
    "                \"EmbeddingConfig: fasttext pretrained embedding not found.\"\n",
    "            )\n",
    "\n",
    "        file_path = Path(self.model_path)\n",
    "        if not file_path.exists():\n",
    "            raise ValueError(\n",
    "                f\"EmbeddingConfig: fasttext pretrained embedding not found. {file_path}\"\n",
    "            )\n",
    "\n",
    "        model = FastText.load(str(file_path))\n",
    "        embedding_data = self.embedding_data(model)\n",
    "        self.vocab_size = embedding_data[\"vocab_size\"]\n",
    "        self.embed_dim = embedding_data[\"embed_dim\"]\n",
    "\n",
    "        return embedding_data[\"embedding_weight\"]\n",
    "\n",
    "\n",
    "class TransformerClassifier(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_dict: dict,\n",
    "        num_classes: int,\n",
    "        nhead: int = 8,\n",
    "        num_layers: int = 2,\n",
    "        dim_feedforward: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "        max_seq_length: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # vocab_size, embed_dim, embedding_weight = self.decode_config(config_dict)\n",
    "        config = EmbeddingConfig(**config_dict)\n",
    "        self.embedding, config = self.build_embedding(config)\n",
    "        self.embed_dim = config.embed_dim\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(self.embed_dim, dropout, max_seq_length)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # 可採用簡單的池化方式，例如取 [CLS] token 的輸出\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, self.embed_dim//2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.embed_dim//2, num_classes),\n",
    "        )\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        return\n",
    "\n",
    "    def build_embedding(self, config: EmbeddingConfig):\n",
    "\n",
    "        if not config.state:\n",
    "            return nn.Embedding(config.vocab_size, config.embed_dim), config\n",
    "\n",
    "        embedding_weight = config.build_embedding_weight()\n",
    "\n",
    "        return (\n",
    "            nn.Embedding.from_pretrained(embedding_weight, freeze=config.freeze),\n",
    "            config,\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (batch_size, seq_len) --> token ids\n",
    "        x = self.embedding(src) * math.sqrt(\n",
    "            self.embed_dim\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        x = self.pos_encoder(x)\n",
    "        # TransformerEncoder 預設輸入 shape: (seq_len, batch_size, embed_dim)\n",
    "        # x = x.transpose(0, 1)\n",
    "        x = self.transformer_encoder(x)  # (seq_len, batch_size, embed_dim)\n",
    "        # 此處假設第一個 token 為 [CLS] token，可用於分類\n",
    "        # cls_token = x[0]  # shape: (batch_size, embed_dim)\n",
    "        # x = x[:, 0, :]  # shape: (batch_size, embed_dim)\n",
    "        x = x.mean(dim=1)  # shape: (batch_size, embed_dim)\n",
    "        logits = self.classifier(x)  # shape: (batch_size, num_classes)\n",
    "        return logits\n",
    "\n",
    "    @staticmethod\n",
    "    def load_from_embedding_pretrained(\n",
    "        embedding_path: str,\n",
    "        num_classes: int,\n",
    "        feeze_pretrained_embedding: bool = True,\n",
    "        nhead: int = 8,\n",
    "        num_layers: int = 2,\n",
    "        dim_feedforward: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "        max_seq_length: int = 512,\n",
    "    ):\n",
    "        embedding_config = EmbeddingConfig.load_embedding_pretrain(\n",
    "            embedding_path, feeze_pretrained_embedding\n",
    "        )\n",
    "\n",
    "        return TransformerClassifier(\n",
    "            asdict(embedding_config),\n",
    "            num_classes=num_classes,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            max_seq_length=max_seq_length,\n",
    "        )\n",
    "    @staticmethod\n",
    "    def build_model(\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        num_classes: int,\n",
    "        nhead: int = 8,\n",
    "        num_layers: int = 2,\n",
    "        dim_feedforward: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "        max_seq_length: int = 512,\n",
    "    ):\n",
    "        embedding_config = EmbeddingConfig.config(\n",
    "            vocab_size=vocab_size, embed_dim=embed_dim\n",
    "        )\n",
    "        return TransformerClassifier(\n",
    "            asdict(embedding_config),\n",
    "            num_classes=num_classes,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            max_seq_length=max_seq_length,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "72474e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # 範例：假設詞彙大小 10000, 嵌入維度 128, 分類類別數 5, 輸入長度 50\n",
    "# batch_size = 16\n",
    "# seq_len = 50\n",
    "# vocab_size = 10000\n",
    "# embed_dim = 128\n",
    "# num_classes = 5\n",
    "\n",
    "# model = TransformerClassifier(vocab_size, embed_dim, num_classes)\n",
    "# sample_input = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "# output = model(sample_input)\n",
    "# print(output.shape)  # 預期：(batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = 10000\n",
    "# embed_dim = 128\n",
    "# model = TransformerClassifier.build_model(\n",
    "#     vocab_size=vocab_size,\n",
    "#     embed_dim=embed_dim,\n",
    "#     num_classes=5,\n",
    "#     nhead=8,\n",
    "#     num_layers=6,\n",
    "#     dim_feedforward=512,\n",
    "#     dropout=0.1,\n",
    "#     max_seq_length=512,\n",
    "# )\n",
    "# print(model)\n",
    "# dummy_input = torch.randint(0, vocab_size, (16, 50))\n",
    "# output = model(dummy_input)\n",
    "# print(output.shape)  # 預期：(batch_size, num_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
