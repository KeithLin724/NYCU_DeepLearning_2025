{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import spacy\n",
    "from spacy.cli import download\n",
    "import gensim\n",
    "from gensim.models import FastText\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8adc429",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData:\n",
    "    # embedding model use fasttext\n",
    "    def __init__(self, train_data:pd.DataFrame, test_data:pd.DataFrame):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.embedding_model_name = None\n",
    "        \n",
    "        self.setup()\n",
    "        return \n",
    "    \n",
    "    # def train_embedding_model(self, result_files:dict,model_type:str=\"skipgram\", embedding_size:int=300):\n",
    "    #     self.embedding_model_name = model_type\n",
    "    #     file_to_train = result_files[\"embedding_training\"]\n",
    "    #     self.model = fasttext.train_unsupervised(file_to_train, model=model_type, minCount=1)\n",
    "        \n",
    "    #     return \n",
    "    \n",
    "    # def load_embedding_model(self, model_path:str):\n",
    "    #     self.embedding_model_name = model_path\n",
    "    #     self.model = fasttext.load_model(model_path)\n",
    "        \n",
    "    #     return\n",
    "    \n",
    "    def setup(self):\n",
    "        # 第一次執行需要下載停用詞資源\n",
    "        download(\"en_core_web_sm\")\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        return\n",
    "    \n",
    "    def clean_text_spacy(self, text:str) -> str:\n",
    "        condition = lambda token : not token.is_stop and not token.is_punct and not token.is_digit\n",
    "        \n",
    "        doc = self.nlp(text)\n",
    "        # 篩選出非停用字、非標點符號、非數字的 token，並轉為小寫\n",
    "        tokens = [token.text.lower() for token in doc if condition(token)]\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    # def clean_text_nltk(self, text: str) -> str:\n",
    "    #     # 轉成小寫\n",
    "    #     text = text.lower()\n",
    "    #     # 移除數字\n",
    "    #     text = re.sub(r'\\d+', '', text)\n",
    "    #     # 移除標點符號\n",
    "    #     text = re.sub(r'[{}]'.format(re.escape(string.punctuation)), '', text)\n",
    "    #     # 移除多餘空白\n",
    "    #     text = text.strip()\n",
    "    #     # 斷詞\n",
    "    #     tokens = text.split()\n",
    "    #     # 移除停用字\n",
    "    #     stop_words = set(stopwords.words(\"english\"))\n",
    "    #     tokens = [word for word in tokens if word not in stop_words]\n",
    "    #     # 回傳清理後的結果\n",
    "    #     return \" \".join(tokens)\n",
    "    \n",
    "    def batch_process_text(self, process_funcs:list[Callable], max_workers:int=None):\n",
    "        result = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [executor.submit(process_func) for process_func in process_funcs]\n",
    "            for future in as_completed(futures):\n",
    "                result.append(future.result())\n",
    "                \n",
    "        return result\n",
    "    \n",
    "    def wrapper_function(self,id_:int, type_:int, headline_in:str, short_description_in:str, label:int, process_func:str):\n",
    "        \n",
    "        def process_it():\n",
    "            headline = process_func(headline_in)\n",
    "            short_description = process_func(short_description_in)\n",
    "            item = \"<HL_S> \" + headline + \" <HL_E> <SD_S> \" +  short_description + \" <SD_E>\"\n",
    "            return {\"id\": id_, \"type\": type_, \"process_text\": item, \"label\": label}\n",
    "        \n",
    "        return process_it\n",
    "    \n",
    "    \n",
    "    def process_all_data(self, process:str, batch_size:int=100, max_workers:int=None):\n",
    "        process_func = self.clean_text_spacy if process == \"spacy\" else self.clean_text_nltk\n",
    "        \n",
    "        train_data_df = self.train_data[[\"id\" , \"headline\", \"short_description\", \"label\"]].copy()\n",
    "        train_data_df[\"type\"] = \"train\"\n",
    "        test_data_df = self.test_data[[\"id\" ,\"headline\", \"short_description\"]].copy()\n",
    "        test_data_df[\"type\"] = \"test\"\n",
    "        test_data_df[\"label\"] = \"IDK\"\n",
    "        \n",
    "        data_df = pd.concat([train_data_df, test_data_df], axis=0)\n",
    "        \n",
    "        \n",
    "        jobs = []\n",
    "        for _, row in tqdm(data_df.iterrows(), total=data_df.shape[0], desc=\"Build Jobs\"):\n",
    "            id_ = row[\"id\"]\n",
    "            type_ = row[\"type\"]\n",
    "            headline = row[\"headline\"]\n",
    "            short_description = row[\"short_description\"]\n",
    "            label = row[\"label\"]\n",
    "            \n",
    "            # 使用 wrapper_function 來包裝 process_func\n",
    "            job = self.wrapper_function(id_, type_, headline, short_description, label , process_func)\n",
    "            jobs.append(job)\n",
    "        \n",
    "        result = []\n",
    "        for batch in tqdm(range(0, len(jobs), batch_size), desc=\"Processing text\"):\n",
    "            batch_jobs = jobs[batch:batch + batch_size]\n",
    "            out = self.batch_process_text(batch_jobs, max_workers)\n",
    "            result.extend(out)\n",
    "        \n",
    "        \n",
    "        output_df = pd.DataFrame(result)\n",
    "        \n",
    "        prefix = f\"temp_{process}\"\n",
    "        \n",
    "        # to temp file for embedding\n",
    "        with open(f\"{prefix}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for text in output_df[\"process_text\"]:\n",
    "                f.write(text + \"\\n\")\n",
    "        \n",
    "        # save processed data\n",
    "        output_df.to_csv(f\"{prefix}.csv\", index=False)\n",
    "        \n",
    "        return {\"embedding_training\" :f\"{prefix}.txt\", \"record\": f\"{prefix}.csv\" }\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_from_folder(folder:str): \n",
    "        path = Path(folder)\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Folder {folder} does not exist.\")\n",
    "        \n",
    "        train_data_json = path / \"News_train.json\"\n",
    "        test_data_json = path / \"News_test.json\"\n",
    "        \n",
    "        with open(train_data_json, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            train_data = [json.loads(line) for line in lines]   \n",
    "            \n",
    "        with open(test_data_json, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            test_data = [json.loads(line) for line in lines]\n",
    "        \n",
    "        train_data = pd.DataFrame(train_data)\n",
    "        test_data = pd.DataFrame(test_data)\n",
    "        \n",
    "        return TextData(train_data, test_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb793da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/keithlin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "text_data = TextData.load_from_folder(\"./2025-deep-learning-hw-2-text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d16a8cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Build Jobs:   0%|          | 0/136608 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Build Jobs: 100%|██████████| 136608/136608 [00:06<00:00, 21841.06it/s]\n",
      "Processing text: 100%|██████████| 137/137 [18:21<00:00,  8.04s/it]\n"
     ]
    }
   ],
   "source": [
    "output = text_data.process_all_data(\"spacy\", batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10704111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding_training': 'temp_spacy.txt', 'record': 'temp_spacy.csv'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fda42cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  18\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   70227 lr:  0.000000 avg.loss:      -nan ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 透過 unsupervised 方法，使用 Skipgram 模型進行訓練\n",
    "model = fasttext.train_unsupervised(\"corpus.txt\", model=\"skipgram\", minCount=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41780f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<END>',\n",
       " '</s>',\n",
       " '<START>',\n",
       " 'is',\n",
       " 'example',\n",
       " 'great',\n",
       " 'data',\n",
       " 'text',\n",
       " 'more',\n",
       " 'with',\n",
       " 'Another',\n",
       " 'embeddings',\n",
       " 'word',\n",
       " 'for',\n",
       " 'FastText',\n",
       " 'sentence',\n",
       " 'an',\n",
       " 'This']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63838fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<END>',\n",
       " '<START>',\n",
       " 'Another',\n",
       " 'FastText',\n",
       " 'This',\n",
       " 'an',\n",
       " 'data',\n",
       " 'embeddings',\n",
       " 'example',\n",
       " 'for',\n",
       " 'great',\n",
       " 'is',\n",
       " 'more',\n",
       " 'sentence',\n",
       " 'text',\n",
       " 'with',\n",
       " 'word'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"corpus.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "lines = [line.strip() for line in lines]\n",
    "lines = sum([line.split() for line in lines] ,[]  )\n",
    "lines = set(lines)\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "733ff1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in the model: ['<END>', '</s>', '<START>', 'is', 'example', 'great', 'data', 'text', 'more', 'with', 'Another', 'embeddings', 'word', 'for', 'FastText', 'sentence', 'an', 'This']\n",
      "Word embedding for 'example': [-4.0063375e-04  5.8068166e-04  1.3081767e-03 -2.6234989e-03\n",
      "  1.9974632e-03 -4.3969785e-04  6.2194240e-04 -2.4216913e-03\n",
      " -1.3190987e-04  2.3871241e-03  2.3822008e-04 -7.0877204e-04\n",
      "  2.4723849e-04  1.5360215e-03  2.9191854e-03 -3.2843975e-04\n",
      "  1.9873954e-04 -5.6091917e-04 -1.1284921e-03  5.3638930e-04\n",
      "  1.6385373e-03 -1.0516634e-03 -8.7366643e-04 -1.5904498e-03\n",
      "  2.8785563e-03 -3.9984708e-04 -1.8464609e-03 -3.6308047e-04\n",
      "  1.3621560e-03  4.3135913e-04  1.6672076e-03 -2.3732992e-04\n",
      "  8.5021317e-04 -8.0895174e-04 -1.0747846e-03  1.0381772e-03\n",
      " -3.3981912e-04 -6.8921386e-04  1.2226522e-03  1.7460980e-04\n",
      "  2.7880720e-05  8.9875195e-04  4.5307167e-04 -1.0518832e-03\n",
      "  2.1610567e-03 -2.9102925e-04  1.5453915e-03  2.1079767e-03\n",
      " -7.3856988e-04 -6.1839382e-04 -5.8842701e-04  2.5627029e-04\n",
      " -2.5520092e-03  1.3258312e-03  2.0650080e-03  2.2573262e-03\n",
      "  1.8445445e-04 -5.2116491e-04  1.3269370e-03  2.5414035e-04\n",
      " -2.3520805e-03  1.6991107e-03 -1.4233940e-03 -2.1433407e-04\n",
      " -1.1014523e-03 -1.1770589e-03  1.2589847e-04  1.6665919e-03\n",
      " -1.5327629e-03 -1.4944751e-03 -1.6604139e-03 -4.1216603e-04\n",
      "  3.7228697e-04  1.8582751e-04  1.0767842e-03  6.0473214e-04\n",
      " -1.3185168e-03  2.2435824e-03 -7.3207886e-04 -1.3554721e-03\n",
      "  9.0434914e-04 -9.2459400e-04 -3.9061997e-05  5.2656629e-04\n",
      "  3.1038374e-04 -7.3104142e-04  8.4974861e-04  1.5620170e-03\n",
      " -1.9828463e-03  2.0573467e-03 -6.5489812e-04  6.6859252e-04\n",
      "  2.0065934e-03  1.7442298e-03  1.9229774e-03 -1.1337992e-03\n",
      " -3.2328742e-04  9.6043694e-04 -8.2094839e-04 -1.5740957e-03]\n",
      "Nearest neighbors for 'fasttext': [(0.44070231914520264, 'text'), (0.26745423674583435, 'FastText'), (0.1307508945465088, 'an'), (0.09248192608356476, 'is'), (0.05365993455052376, 'data'), (0.018413128331303596, 'with'), (0.017535213381052017, '<END>'), (-0.022479280829429626, 'This'), (-0.0274488627910614, '</s>'), (-0.031967971473932266, 'word')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 查看模型中的單詞列表\n",
    "print(\"Words in the model:\", model.words)\n",
    "\n",
    "# 取得某個單詞的詞向量\n",
    "word_embedding = model.get_word_vector(\"example\")\n",
    "print(\"Word embedding for 'example':\", word_embedding)\n",
    "\n",
    "# 使用模型預測句子中最相似的單詞（例如：以內建的 nearest neighbor 方法）\n",
    "nearest_neighbors = model.get_nearest_neighbors(\"Fasttext\")\n",
    "print(\"Nearest neighbors for 'fasttext':\", nearest_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73bfe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/keithlin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46446d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 請先安裝 spaCy 及英文模型： pip install spacy && python -m spacy download en_core_web_sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c38ed837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試範例\n",
    "sample_text = \"FastText is great for word embeddings! In 2025, we are using it for text classification.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4330386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fasttext great word embeddings text classification'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text_spacy(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57a90261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text: fasttext great word embeddings using text classification\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cleaned_text = clean_text(sample_text)\n",
    "print(\"Cleaned text:\", cleaned_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
